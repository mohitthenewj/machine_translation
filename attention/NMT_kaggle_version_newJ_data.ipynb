{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "popular-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import io\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "        \n",
    "PATH = \"../data_text/pmindia.v1.hi-en.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "lyric-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(PATH,sep='\\t', encoding='utf-8', names = [\"english_sentence\", \"hindi_sentence\"] )\n",
    "\n",
    "# df.to_csv(\"../data_text/pmindia.v1.hi-en.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "alternate-torture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An advance is placed with the Medical Superint...</td>\n",
       "      <td>अग्रिम धन राशि इन अस्पतालों को चिकित्सा निरीक्...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since the DoHFW provides funds to the hospital...</td>\n",
       "      <td>चूंकि स्वास्थ्य एवं परिवार कल्याण विभाग अस्पता...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RAN functions can, therefore, be vested in DoHFW.</td>\n",
       "      <td>इस तरह आरएएन का कामकाज स्वास्थ्य एवं परिवार कल...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Managing Committee of RAN Society will meet to...</td>\n",
       "      <td>आरएएन, सोसायटी की प्रबंध समिति सोसायटी पंजीकरण...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In addition to this, Health Minister’s Cancer ...</td>\n",
       "      <td>इसके अलावा स्वास्थ्य मंत्री के कैंसर रोगी निधि...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    english_sentence  \\\n",
       "0  An advance is placed with the Medical Superint...   \n",
       "1  Since the DoHFW provides funds to the hospital...   \n",
       "2  RAN functions can, therefore, be vested in DoHFW.   \n",
       "3  Managing Committee of RAN Society will meet to...   \n",
       "4  In addition to this, Health Minister’s Cancer ...   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  अग्रिम धन राशि इन अस्पतालों को चिकित्सा निरीक्...  \n",
       "1  चूंकि स्वास्थ्य एवं परिवार कल्याण विभाग अस्पता...  \n",
       "2  इस तरह आरएएन का कामकाज स्वास्थ्य एवं परिवार कल...  \n",
       "3  आरएएन, सोसायटी की प्रबंध समिति सोसायटी पंजीकरण...  \n",
       "4  इसके अलावा स्वास्थ्य मंत्री के कैंसर रोगी निधि...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "velvet-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    return w\n",
    "\n",
    "\n",
    "def hindi_preprocess_sentence(w):\n",
    "    w = w.rstrip().strip()\n",
    "    return w\n",
    "def create_dataset(path=PATH):\n",
    "    lines=pd.read_csv(path, encoding='utf-8')\n",
    "    lines=lines.dropna()\n",
    "    lines = lines[lines['source']=='ted']\n",
    "    en = []\n",
    "    hd = []\n",
    "    for i, j in zip(lines['english_sentence'], lines['hindi_sentence']):\n",
    "        en_1 = [preprocess_sentence(w) for w in i.split(' ')]\n",
    "        en_1.append('<end>')\n",
    "        en_1.insert(0, '<start>')\n",
    "        hd_1 = [hindi_preprocess_sentence(w) for w in j.split(' ')]\n",
    "        hd_1.append('<end>')\n",
    "        hd_1.insert(0, '<start>')\n",
    "        en.append(en_1)\n",
    "        hd.append(hd_1)\n",
    "    return hd, en\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "def load_dataset(path=PATH):\n",
    "    targ_lang, inp_lang = create_dataset(path)\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "extraordinary-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "resistant-disability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39881, 23), (39881, 33))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape, target_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "collectible-clark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 23)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_targ, max_length_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "transparent-suspension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31904 31904 7977 7977\n"
     ]
    }
   ],
   "source": [
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
    "\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "extensive-technical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "5 ----> to\n",
      "78 ----> some\n",
      "6 ----> of\n",
      "3 ----> the\n",
      "2130 ----> poorest\n",
      "40 ----> people\n",
      "26 ----> on\n",
      "3 ----> the\n",
      "2140 ----> planet .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "105 ----> दुनिया\n",
      "4 ----> के\n",
      "33 ----> कुछ\n",
      "101 ----> सबसे\n",
      "527 ----> गरीब\n",
      "61 ----> लोगों\n",
      "72 ----> तक\n",
      "5447 ----> पहुँचाया\n",
      "36 ----> है।\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "    \n",
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "actual-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 128\n",
    "units = 256\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "    x = self.embedding(x)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "    output, state = self.gru(x)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    x = self.fc(output)\n",
    "    return x, state, attention_weights\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "#   print(type(mask))\n",
    "  loss_ *= mask\n",
    "  return tf.reduce_mean(loss_)\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fresh-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "    # Teacher forcing\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))      \n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "precious-depth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.0001\n",
      "Epoch 1 Batch 100 Loss 1.9614\n",
      "Epoch 1 Batch 200 Loss 1.7728\n",
      "Epoch 1 Batch 300 Loss 1.6786\n",
      "Epoch 1 Batch 400 Loss 1.6685\n",
      "Epoch 1 Loss 1.9444\n",
      "Time taken for 1 epoch 402.2700490951538 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.9717\n",
      "Epoch 2 Batch 100 Loss 1.7873\n",
      "Epoch 2 Batch 200 Loss 1.7760\n",
      "Epoch 2 Batch 300 Loss 1.6777\n",
      "Epoch 2 Batch 400 Loss 1.5780\n",
      "Epoch 2 Loss 1.7386\n",
      "Time taken for 1 epoch 370.02228474617004 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "    if batch % 100 == 0:\n",
    "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "familiar-double",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: politicians do not have permission to do what needs to be done .\n",
      "Predicted translation: और यह एक तरह के लिए <end> \n"
     ]
    }
   ],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_inp,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence\n",
    "def translate(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "translate(u'politicians do not have permission to do what needs to be done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-queen",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
