{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "popular-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import digits\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import io\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "        \n",
    "PATH = \"../data_text/Hindi_English_Truncated_Corpus.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "lyric-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(PATH,sep='\\t', encoding='utf-8', names = [\"english_sentence\", \"hindi_sentence\"] )\n",
    "\n",
    "# df.to_csv(\"../data_text/pmindia.v1.hi-en.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "alternate-torture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An advance is placed with the Medical Superint...</td>\n",
       "      <td>अग्रिम धन राशि इन अस्पतालों को चिकित्सा निरीक्...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since the DoHFW provides funds to the hospital...</td>\n",
       "      <td>चूंकि स्वास्थ्य एवं परिवार कल्याण विभाग अस्पता...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RAN functions can, therefore, be vested in DoHFW.</td>\n",
       "      <td>इस तरह आरएएन का कामकाज स्वास्थ्य एवं परिवार कल...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Managing Committee of RAN Society will meet to...</td>\n",
       "      <td>आरएएन, सोसायटी की प्रबंध समिति सोसायटी पंजीकरण...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In addition to this, Health Minister’s Cancer ...</td>\n",
       "      <td>इसके अलावा स्वास्थ्य मंत्री के कैंसर रोगी निधि...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    english_sentence  \\\n",
       "0  An advance is placed with the Medical Superint...   \n",
       "1  Since the DoHFW provides funds to the hospital...   \n",
       "2  RAN functions can, therefore, be vested in DoHFW.   \n",
       "3  Managing Committee of RAN Society will meet to...   \n",
       "4  In addition to this, Health Minister’s Cancer ...   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  अग्रिम धन राशि इन अस्पतालों को चिकित्सा निरीक्...  \n",
       "1  चूंकि स्वास्थ्य एवं परिवार कल्याण विभाग अस्पता...  \n",
       "2  इस तरह आरएएन का कामकाज स्वास्थ्य एवं परिवार कल...  \n",
       "3  आरएएन, सोसायटी की प्रबंध समिति सोसायटी पंजीकरण...  \n",
       "4  इसके अलावा स्वास्थ्य मंत्री के कैंसर रोगी निधि...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "velvet-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.rstrip().strip()\n",
    "    return w\n",
    "\n",
    "\n",
    "def hindi_preprocess_sentence(w):\n",
    "    w = w.rstrip().strip()\n",
    "    return w\n",
    "def create_dataset(path=PATH):\n",
    "    lines=pd.read_csv(path, encoding='utf-8')\n",
    "    lines=lines.dropna()\n",
    "    lines = lines[lines['source']=='ted']\n",
    "    en = []\n",
    "    hd = []\n",
    "    for i, j in zip(lines['english_sentence'], lines['hindi_sentence']):\n",
    "        en_1 = [preprocess_sentence(w) for w in i.split(' ')]\n",
    "        en_1.append('<end>')\n",
    "        en_1.insert(0, '<start>')\n",
    "        hd_1 = [hindi_preprocess_sentence(w) for w in j.split(' ')]\n",
    "        hd_1.append('<end>')\n",
    "        hd_1.insert(0, '<start>')\n",
    "        en.append(en_1)\n",
    "        hd.append(hd_1)\n",
    "    return hd, en\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "def load_dataset(path=PATH):\n",
    "    targ_lang, inp_lang = create_dataset(path)\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "extraordinary-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "resistant-disability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39881, 23), (39881, 33))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape, target_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "collectible-clark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 23)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_targ, max_length_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "transparent-suspension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31904 31904 7977 7977\n"
     ]
    }
   ],
   "source": [
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)\n",
    "\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "extensive-technical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "9 ----> that\n",
      "76 ----> really\n",
      "2311 ----> prefer\n",
      "3 ----> the\n",
      "264 ----> end\n",
      "6 ----> of\n",
      "11363 ----> texts ,\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "20 ----> जो\n",
      "1337 ----> वाक्य\n",
      "51 ----> या\n",
      "2695 ----> पृष्ठ\n",
      "4 ----> के\n",
      "354 ----> अंत\n",
      "5 ----> में\n",
      "409 ----> आते\n",
      "32 ----> हैं,\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "    \n",
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "actual-native",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 128\n",
    "units = 256\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    return context_vector, attention_weights\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "    x = self.embedding(x)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "    output, state = self.gru(x)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    x = self.fc(output)\n",
    "    return x, state, attention_weights\n",
    "\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "#   print(type(mask))\n",
    "  loss_ *= mask\n",
    "  return tf.reduce_mean(loss_)\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fresh-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "    # Teacher forcing\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "  optimizer.apply_gradients(zip(gradients, variables))      \n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "precious-depth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.7485\n",
      "Epoch 1 Batch 100 Loss 2.0161\n",
      "Epoch 1 Batch 200 Loss 1.8809\n",
      "Epoch 1 Batch 300 Loss 1.8865\n",
      "Epoch 1 Loss 1.9453\n",
      "Time taken for 1 epoch 401.7105362415314 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.8744\n",
      "Epoch 2 Batch 100 Loss 1.6609\n",
      "Epoch 2 Batch 200 Loss 1.7167\n",
      "Epoch 2 Batch 300 Loss 1.7101\n",
      "Epoch 2 Batch 400 Loss 1.6359\n",
      "Epoch 2 Loss 1.7437\n",
      "Time taken for 1 epoch 367.8652675151825 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.6424\n",
      "Epoch 3 Batch 100 Loss 1.8386\n",
      "Epoch 3 Batch 200 Loss 1.6650\n",
      "Epoch 3 Batch 300 Loss 1.6430\n",
      "Epoch 3 Batch 400 Loss 1.4835\n",
      "Epoch 3 Loss 1.6458\n",
      "Time taken for 1 epoch 366.37940526008606 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.5123\n",
      "Epoch 4 Batch 100 Loss 1.6303\n",
      "Epoch 4 Batch 200 Loss 1.6166\n",
      "Epoch 4 Batch 300 Loss 1.5113\n",
      "Epoch 4 Batch 400 Loss 1.5433\n",
      "Epoch 4 Loss 1.5537\n",
      "Time taken for 1 epoch 366.91216921806335 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.5321\n",
      "Epoch 5 Batch 100 Loss 1.4973\n",
      "Epoch 5 Batch 200 Loss 1.4603\n",
      "Epoch 5 Batch 300 Loss 1.4860\n",
      "Epoch 5 Batch 400 Loss 1.4598\n",
      "Epoch 5 Loss 1.4613\n",
      "Time taken for 1 epoch 399.6376097202301 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.2617\n",
      "Epoch 6 Batch 100 Loss 1.3716\n",
      "Epoch 6 Batch 200 Loss 1.3001\n",
      "Epoch 6 Batch 300 Loss 1.3423\n",
      "Epoch 6 Batch 400 Loss 1.4450\n",
      "Epoch 6 Loss 1.3689\n",
      "Time taken for 1 epoch 422.80787086486816 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.3247\n",
      "Epoch 7 Batch 100 Loss 1.1331\n",
      "Epoch 7 Batch 200 Loss 1.3525\n",
      "Epoch 7 Batch 300 Loss 1.4566\n",
      "Epoch 7 Batch 400 Loss 1.3487\n",
      "Epoch 7 Loss 1.2807\n",
      "Time taken for 1 epoch 386.5340142250061 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.0849\n",
      "Epoch 8 Batch 100 Loss 1.1611\n",
      "Epoch 8 Batch 200 Loss 1.1794\n",
      "Epoch 8 Batch 300 Loss 1.1584\n",
      "Epoch 8 Batch 400 Loss 1.1609\n",
      "Epoch 8 Loss 1.1936\n",
      "Time taken for 1 epoch 367.5442452430725 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.0672\n",
      "Epoch 9 Batch 100 Loss 1.0839\n",
      "Epoch 9 Batch 200 Loss 1.2350\n",
      "Epoch 9 Batch 300 Loss 1.1355\n",
      "Epoch 9 Batch 400 Loss 1.2154\n",
      "Epoch 9 Loss 1.1100\n",
      "Time taken for 1 epoch 367.322452545166 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.1413\n",
      "Epoch 10 Batch 100 Loss 0.9126\n",
      "Epoch 10 Batch 200 Loss 0.9825\n",
      "Epoch 10 Batch 300 Loss 0.9852\n",
      "Epoch 10 Batch 400 Loss 1.0334\n",
      "Epoch 10 Loss 1.0296\n",
      "Time taken for 1 epoch 365.46054911613464 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.8253\n",
      "Epoch 11 Batch 100 Loss 0.9492\n",
      "Epoch 11 Batch 200 Loss 0.8956\n",
      "Epoch 11 Batch 300 Loss 0.9001\n",
      "Epoch 11 Batch 400 Loss 0.8907\n",
      "Epoch 11 Loss 0.9525\n",
      "Time taken for 1 epoch 365.7812833786011 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.8952\n",
      "Epoch 12 Batch 100 Loss 0.8602\n",
      "Epoch 12 Batch 200 Loss 0.8275\n",
      "Epoch 12 Batch 300 Loss 0.9747\n",
      "Epoch 12 Batch 400 Loss 0.8114\n",
      "Epoch 12 Loss 0.8802\n",
      "Time taken for 1 epoch 365.18299651145935 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.8716\n",
      "Epoch 13 Batch 100 Loss 0.7681\n",
      "Epoch 13 Batch 200 Loss 0.8509\n",
      "Epoch 13 Batch 300 Loss 0.9237\n",
      "Epoch 13 Batch 400 Loss 0.8798\n",
      "Epoch 13 Loss 0.8130\n",
      "Time taken for 1 epoch 365.6633858680725 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.8102\n",
      "Epoch 14 Batch 100 Loss 0.7278\n",
      "Epoch 14 Batch 200 Loss 0.7110\n",
      "Epoch 14 Batch 300 Loss 0.8661\n",
      "Epoch 14 Batch 400 Loss 0.7324\n",
      "Epoch 14 Loss 0.7513\n",
      "Time taken for 1 epoch 369.24601793289185 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.6784\n",
      "Epoch 15 Batch 100 Loss 0.6453\n",
      "Epoch 15 Batch 200 Loss 0.6947\n",
      "Epoch 15 Batch 300 Loss 0.6846\n",
      "Epoch 15 Batch 400 Loss 0.5904\n",
      "Epoch 15 Loss 0.6962\n",
      "Time taken for 1 epoch 430.2738604545593 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.5839\n",
      "Epoch 16 Batch 100 Loss 0.5789\n",
      "Epoch 16 Batch 200 Loss 0.6949\n",
      "Epoch 16 Batch 300 Loss 0.6497\n",
      "Epoch 16 Batch 400 Loss 0.6395\n",
      "Epoch 16 Loss 0.6455\n",
      "Time taken for 1 epoch 419.8731281757355 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.5817\n",
      "Epoch 17 Batch 100 Loss 0.5618\n",
      "Epoch 17 Batch 200 Loss 0.5602\n",
      "Epoch 17 Batch 300 Loss 0.5935\n",
      "Epoch 17 Batch 400 Loss 0.6832\n",
      "Epoch 17 Loss 0.6011\n",
      "Time taken for 1 epoch 367.84702014923096 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.4632\n",
      "Epoch 18 Batch 100 Loss 0.5834\n",
      "Epoch 18 Batch 200 Loss 0.5361\n",
      "Epoch 18 Batch 300 Loss 0.5969\n",
      "Epoch 18 Batch 400 Loss 0.5850\n",
      "Epoch 18 Loss 0.5579\n",
      "Time taken for 1 epoch 365.1563000679016 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.5351\n",
      "Epoch 19 Batch 100 Loss 0.4967\n",
      "Epoch 19 Batch 200 Loss 0.5488\n",
      "Epoch 19 Batch 300 Loss 0.4538\n",
      "Epoch 19 Batch 400 Loss 0.4817\n",
      "Epoch 19 Loss 0.5187\n",
      "Time taken for 1 epoch 364.58261919021606 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.4540\n",
      "Epoch 20 Batch 100 Loss 0.4141\n",
      "Epoch 20 Batch 200 Loss 0.4488\n",
      "Epoch 20 Batch 300 Loss 0.5883\n",
      "Epoch 20 Batch 400 Loss 0.4886\n",
      "Epoch 20 Loss 0.4832\n",
      "Time taken for 1 epoch 364.75097131729126 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.4225\n",
      "Epoch 21 Batch 100 Loss 0.4259\n",
      "Epoch 21 Batch 200 Loss 0.4173\n",
      "Epoch 21 Batch 300 Loss 0.4896\n",
      "Epoch 21 Batch 400 Loss 0.4962\n",
      "Epoch 21 Loss 0.4493\n",
      "Time taken for 1 epoch 367.958615064621 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.3509\n",
      "Epoch 22 Batch 100 Loss 0.4037\n",
      "Epoch 22 Batch 200 Loss 0.4361\n",
      "Epoch 22 Batch 300 Loss 0.5032\n",
      "Epoch 22 Batch 400 Loss 0.3470\n",
      "Epoch 22 Loss 0.4181\n",
      "Time taken for 1 epoch 368.39988112449646 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.3889\n",
      "Epoch 23 Batch 100 Loss 0.4489\n",
      "Epoch 23 Batch 200 Loss 0.3977\n",
      "Epoch 23 Batch 300 Loss 0.4201\n",
      "Epoch 23 Batch 400 Loss 0.4192\n",
      "Epoch 23 Loss 0.3901\n",
      "Time taken for 1 epoch 367.0871887207031 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.3799\n",
      "Epoch 24 Batch 100 Loss 0.3726\n",
      "Epoch 24 Batch 200 Loss 0.3925\n",
      "Epoch 24 Batch 300 Loss 0.2877\n",
      "Epoch 24 Batch 400 Loss 0.4350\n",
      "Epoch 24 Loss 0.3631\n",
      "Time taken for 1 epoch 400.6523640155792 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.3020\n",
      "Epoch 25 Batch 100 Loss 0.2903\n",
      "Epoch 25 Batch 200 Loss 0.3380\n",
      "Epoch 25 Batch 300 Loss 0.4148\n",
      "Epoch 25 Batch 400 Loss 0.3015\n",
      "Epoch 25 Loss 0.3379\n",
      "Time taken for 1 epoch 425.063768863678 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.2450\n",
      "Epoch 26 Batch 100 Loss 0.2955\n",
      "Epoch 26 Batch 200 Loss 0.3345\n",
      "Epoch 26 Batch 300 Loss 0.3578\n",
      "Epoch 26 Batch 400 Loss 0.3026\n",
      "Epoch 26 Loss 0.3151\n",
      "Time taken for 1 epoch 373.9851071834564 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.2403\n",
      "Epoch 27 Batch 100 Loss 0.3250\n",
      "Epoch 27 Batch 200 Loss 0.2836\n",
      "Epoch 27 Batch 300 Loss 0.3104\n",
      "Epoch 27 Batch 400 Loss 0.3071\n",
      "Epoch 27 Loss 0.2943\n",
      "Time taken for 1 epoch 344.78878688812256 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.2834\n",
      "Epoch 28 Batch 100 Loss 0.2622\n",
      "Epoch 28 Batch 200 Loss 0.2394\n",
      "Epoch 28 Batch 300 Loss 0.3122\n",
      "Epoch 28 Batch 400 Loss 0.2576\n",
      "Epoch 28 Loss 0.2765\n",
      "Time taken for 1 epoch 344.8169734477997 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.2308\n",
      "Epoch 29 Batch 100 Loss 0.2572\n",
      "Epoch 29 Batch 200 Loss 0.2389\n",
      "Epoch 29 Batch 400 Loss 0.2481\n",
      "Epoch 29 Loss 0.2539\n",
      "Time taken for 1 epoch 344.837021112442 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.1943\n",
      "Epoch 30 Batch 100 Loss 0.2231\n",
      "Epoch 30 Batch 200 Loss 0.2204\n",
      "Epoch 30 Batch 300 Loss 0.2367\n",
      "Epoch 30 Batch 400 Loss 0.2202\n",
      "Epoch 30 Loss 0.2385\n",
      "Time taken for 1 epoch 343.4863359928131 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.2310\n",
      "Epoch 31 Batch 100 Loss 0.2037\n",
      "Epoch 31 Batch 200 Loss 0.2017\n",
      "Epoch 31 Batch 300 Loss 0.2119\n",
      "Epoch 31 Batch 400 Loss 0.2259\n",
      "Epoch 31 Loss 0.2237\n",
      "Time taken for 1 epoch 343.53263878822327 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.2465\n",
      "Epoch 32 Batch 100 Loss 0.2137\n",
      "Epoch 32 Batch 200 Loss 0.1969\n",
      "Epoch 32 Batch 300 Loss 0.2096\n",
      "Epoch 32 Batch 400 Loss 0.1771\n",
      "Epoch 32 Loss 0.2058\n",
      "Time taken for 1 epoch 344.0589921474457 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.1911\n",
      "Epoch 33 Batch 100 Loss 0.1481\n",
      "Epoch 33 Batch 200 Loss 0.1718\n",
      "Epoch 33 Batch 300 Loss 0.1874\n",
      "Epoch 33 Batch 400 Loss 0.1760\n",
      "Epoch 33 Loss 0.1948\n",
      "Time taken for 1 epoch 345.121896982193 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.1831\n",
      "Epoch 34 Batch 100 Loss 0.1866\n",
      "Epoch 34 Batch 200 Loss 0.1488\n",
      "Epoch 34 Batch 300 Loss 0.2022\n",
      "Epoch 34 Batch 400 Loss 0.1901\n",
      "Epoch 34 Loss 0.1808\n",
      "Time taken for 1 epoch 373.17021083831787 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.1612\n",
      "Epoch 35 Batch 100 Loss 0.1658\n",
      "Epoch 35 Batch 200 Loss 0.1643\n",
      "Epoch 35 Batch 300 Loss 0.1546\n",
      "Epoch 35 Batch 400 Loss 0.1507\n",
      "Epoch 35 Loss 0.1697\n",
      "Time taken for 1 epoch 393.4397814273834 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.1417\n",
      "Epoch 36 Batch 100 Loss 0.1320\n",
      "Epoch 36 Batch 200 Loss 0.1668\n",
      "Epoch 36 Batch 300 Loss 0.1655\n",
      "Epoch 36 Batch 400 Loss 0.1508\n",
      "Epoch 36 Loss 0.1636\n",
      "Time taken for 1 epoch 373.49631905555725 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.1255\n",
      "Epoch 37 Batch 100 Loss 0.1490\n",
      "Epoch 37 Batch 200 Loss 0.1484\n",
      "Epoch 37 Batch 300 Loss 0.1249\n",
      "Epoch 37 Batch 400 Loss 0.1269\n",
      "Epoch 37 Loss 0.1490\n",
      "Time taken for 1 epoch 343.3268885612488 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.1129\n",
      "Epoch 38 Batch 100 Loss 0.1640\n",
      "Epoch 38 Batch 200 Loss 0.1611\n",
      "Epoch 38 Batch 300 Loss 0.1768\n",
      "Epoch 38 Batch 400 Loss 0.1193\n",
      "Epoch 38 Loss 0.1404\n",
      "Time taken for 1 epoch 340.3438878059387 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Batch 0 Loss 0.1261\n",
      "Epoch 39 Batch 100 Loss 0.1371\n",
      "Epoch 39 Batch 200 Loss 0.1188\n",
      "Epoch 39 Batch 300 Loss 0.1520\n",
      "Epoch 39 Batch 400 Loss 0.1213\n",
      "Epoch 39 Loss 0.1326\n",
      "Time taken for 1 epoch 339.09317088127136 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.1310\n",
      "Epoch 40 Batch 100 Loss 0.1129\n",
      "Epoch 40 Batch 200 Loss 0.1310\n",
      "Epoch 40 Batch 300 Loss 0.1199\n",
      "Epoch 40 Batch 400 Loss 0.1264\n",
      "Epoch 40 Loss 0.1228\n",
      "Time taken for 1 epoch 339.04984307289124 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.1144\n",
      "Epoch 41 Batch 100 Loss 0.0783\n",
      "Epoch 41 Batch 200 Loss 0.1092\n",
      "Epoch 41 Batch 300 Loss 0.1276\n",
      "Epoch 41 Batch 400 Loss 0.1229\n",
      "Epoch 41 Loss 0.1120\n",
      "Time taken for 1 epoch 338.9287736415863 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.0970\n",
      "Epoch 42 Batch 100 Loss 0.1093\n",
      "Epoch 42 Batch 200 Loss 0.0742\n",
      "Epoch 42 Batch 300 Loss 0.1278\n",
      "Epoch 42 Batch 400 Loss 0.1246\n",
      "Epoch 42 Loss 0.1050\n",
      "Time taken for 1 epoch 339.2525441646576 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.1126\n",
      "Epoch 43 Batch 100 Loss 0.0954\n",
      "Epoch 43 Batch 200 Loss 0.0862\n",
      "Epoch 43 Batch 300 Loss 0.1215\n",
      "Epoch 43 Batch 400 Loss 0.1072\n",
      "Epoch 43 Loss 0.1003\n",
      "Time taken for 1 epoch 339.4851393699646 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.0959\n",
      "Epoch 44 Batch 100 Loss 0.0903\n",
      "Epoch 44 Batch 200 Loss 0.1120\n",
      "Epoch 44 Batch 300 Loss 0.1035\n",
      "Epoch 44 Batch 400 Loss 0.1078\n",
      "Epoch 44 Loss 0.0952\n",
      "Time taken for 1 epoch 357.02954745292664 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.0885\n",
      "Epoch 45 Batch 100 Loss 0.1003\n",
      "Epoch 45 Batch 200 Loss 0.1147\n",
      "Epoch 45 Batch 300 Loss 0.1019\n",
      "Epoch 45 Batch 400 Loss 0.0824\n",
      "Epoch 45 Loss 0.0930\n",
      "Time taken for 1 epoch 410.255978345871 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.0697\n",
      "Epoch 46 Batch 100 Loss 0.0782\n",
      "Epoch 46 Batch 200 Loss 0.0960\n",
      "Epoch 46 Batch 300 Loss 0.0848\n",
      "Epoch 46 Batch 400 Loss 0.0651\n",
      "Epoch 46 Loss 0.0861\n",
      "Time taken for 1 epoch 396.651162147522 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.0859\n",
      "Epoch 47 Batch 100 Loss 0.0783\n",
      "Epoch 47 Batch 200 Loss 0.0769\n",
      "Epoch 47 Batch 300 Loss 0.1111\n",
      "Epoch 47 Batch 400 Loss 0.0909\n",
      "Epoch 47 Loss 0.0803\n",
      "Time taken for 1 epoch 357.1990737915039 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.0698\n",
      "Epoch 48 Batch 100 Loss 0.0853\n",
      "Epoch 48 Batch 200 Loss 0.0643\n",
      "Epoch 48 Batch 300 Loss 0.0722\n",
      "Epoch 48 Batch 400 Loss 0.0712\n",
      "Epoch 48 Loss 0.0754\n",
      "Time taken for 1 epoch 357.2768814563751 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.0658\n",
      "Epoch 49 Batch 100 Loss 0.0594\n",
      "Epoch 49 Batch 200 Loss 0.0548\n",
      "Epoch 49 Batch 300 Loss 0.0773\n",
      "Epoch 49 Batch 400 Loss 0.0856\n",
      "Epoch 49 Loss 0.0739\n",
      "Time taken for 1 epoch 357.4337794780731 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.0605\n",
      "Epoch 50 Batch 100 Loss 0.0564\n",
      "Epoch 50 Batch 200 Loss 0.0926\n",
      "Epoch 50 Batch 300 Loss 0.0606\n",
      "Epoch 50 Batch 400 Loss 0.0650\n",
      "Epoch 50 Loss 0.0698\n",
      "Time taken for 1 epoch 357.1262993812561 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "    if batch % 100 == 0:\n",
    "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                     batch,\n",
    "                                                     batch_loss.numpy()))\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "familiar-double",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: politicians do not have permission to do what needs to be done .\n",
      "Predicted translation: यह तो बेहतर नहीं होगा अब हमें खाना निकल रहा है कैसे तो नहीं होना चाहिए जा रहा है कैसे तो सही होना नहीं है कैसे दिखते क्या कर सकते। <end> \n"
     ]
    }
   ],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_inp,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence\n",
    "def translate(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "descending-marriage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The timeline required for this is one year.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['english_sentence'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "analyzed-queen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: the time required for this is one year .\n",
      "Predicted translation: इस साल की प्रक्रिया से थोडा सा सैन्य उत्सव एक लय पर काम कर रहा है. <end> \n"
     ]
    }
   ],
   "source": [
    "translate(u'The time required for this is one year.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-bermuda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-excerpt",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-parks",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-update",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
