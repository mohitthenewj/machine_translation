{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "numerous-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark_start = '<start> '\n",
    "# mark_end = ' <end>'\n",
    "# sp = []\n",
    "# for line in tqdm(df[df.columns[1]]):\n",
    "#     output_sentence =  mark_start +  line + mark_end\n",
    "#     sp.append(output_sentence)\n",
    "\n",
    "# en = []\n",
    "# for line in tqdm(df[df.columns[0]]):\n",
    "#     output_sentence =  mark_start +  line + mark_end\n",
    "#     en.append(output_sentence)\n",
    "\n",
    "\n",
    "# en = [[preprocess_sentence(w) for w in l.split('\\t')] for l in en]\n",
    "# sp = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in sp]\n",
    "\n",
    "# en = tuple([i[0] for i in en])\n",
    "# sp = tuple([i[0] for i in sp])\n",
    "\n",
    "# numberOfHindiCharacters = 128;\n",
    "# unicodeShift = 0x0900;\n",
    "# hindiAlphabet = [];\n",
    "# for i in range(numberOfHindiCharacters):\n",
    "#     print(unicodedata.normalize('NFKD', \"\\\\u0\"+str(unicodeShift + i)))\n",
    "# #     hindiAlphabet.appned(\"\\\\u0\" + (unicodeShift + i).toString(16))\n",
    "\n",
    "\n",
    "# # Download the file\n",
    "# path_to_zip = tf.keras.utils.get_file(\n",
    "#     'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "#     extract=True)\n",
    "\n",
    "# path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "increasing-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "preliminary-maple",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data_text/pmindia.v1.hi-en.tsv', sep = '\\t', header =0, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "naval-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_org = df[df.columns[0]]\n",
    "sp_org = df[df.columns[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "raised-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "#     print(f'unicode_to_ascii >> {s}')\n",
    "#     print()\n",
    "    return s\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "#     print(f'preprocess_sentence input >> {w}')\n",
    "    \n",
    "#     print()\n",
    "    w = w.lower().strip()\n",
    "    w = re.sub('\\u200d', '', w)\n",
    "    w = re.sub(r\"([?.!,¿।])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "#     w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "                        \n",
    "#     print(f'preprocess_sentence output >> {w}')\n",
    "#     print()\n",
    "    \n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "qualified-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(zipped_data, num_examples):\n",
    "    lines = list(zipped_data)\n",
    "#     print(lines[-10])\n",
    "#     print(f'org data for create_dataset is >> {lines[:num_examples]}')\n",
    "#     print()\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l]  for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "linear-figure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> commissioners of east , north and south municipal corporations of delhi; <end>\n",
      "<start> दिल्ली के पूर्वी , उत्तरी और दक्षिणी नगर निगमों के आयुक्त; <end>\n"
     ]
    }
   ],
   "source": [
    "num_examples = 200\n",
    "en, sp = create_dataset(zip(en_org,sp_org), None)\n",
    "print(en[-10])\n",
    "print(sp[-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "sticky-bride",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> the prime minister suggested that there should be a common school syllabus on environmental subjects across the world , both in developed and developing countries , so that the younger generation grows up with common goals in the battle against climate change . <end>\n",
      "<start> प्रधानमंत्री ने सुझाव दिया कि विश्व भर में विकसित और विकासशील देशों में पर्यावरण से जुड़े विषयों पर साझा स्कूल पाठ्यक्रम होना चाहिए ताकि जलवायु परिवर्तन के विरुद्ध संघर्ष में साझा लक्ष्यों के साथ युवा पीढ़ी का विकास हो । <end>\n"
     ]
    }
   ],
   "source": [
    "print(en[-189])\n",
    "print(sp[-189])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "educated-default",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> he said the house has an opportunity to leave behind many old traditions , and establish new ones . <end>\n",
      "<start> उन्होंने कहा कि सदन के पास कई पुरानी परंपराओं को छोड़कर नई परंपराएं अपनाने का मौका है । <end>\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_org[20]))\n",
    "print(preprocess_sentence(sp_org[20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "adult-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "def load_dataset(data, num_examples):\n",
    "    # creating cleaned input, output pairs\n",
    "    targ_lang, inp_lang = create_dataset(data, num_examples)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "noted-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "# num_examples = len(en)\n",
    "num_examples = 400\n",
    "\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(zip(en_org,sp_org), num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "chief-burns",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320 320 80 80\n"
     ]
    }
   ],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "civil-default",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56830, 56830)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en), len(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "sapphire-hayes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "2 ----> <start>\n",
      "501 ----> एनसीएमसी\n",
      "26 ----> एक\n",
      "1234 ----> बहुस्तरीय\n",
      "502 ----> समन्वय\n",
      "114 ----> केंद्र\n",
      "9 ----> है\n",
      "363 ----> जिसे\n",
      "1235 ----> आपात\n",
      "364 ----> स्थिति\n",
      "1 ----> के\n",
      "91 ----> दौरान\n",
      "1236 ----> अंतर-एजेंसी\n",
      "502 ----> समन्वय\n",
      "69 ----> प्रदान\n",
      "21 ----> करने\n",
      "8 ----> और\n",
      "38 ----> लोगों\n",
      "7 ----> को\n",
      "365 ----> खतरे\n",
      "1 ----> के\n",
      "135 ----> प्रति\n",
      "1237 ----> अलर्ट\n",
      "21 ----> करने\n",
      "1 ----> के\n",
      "16 ----> लिए\n",
      "1238 ----> डिजाइन\n",
      "24 ----> किया\n",
      "86 ----> गया\n",
      "9 ----> है\n",
      "4 ----> ।\n",
      "3 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "3 ----> <start>\n",
      "1 ----> the\n",
      "418 ----> ncmc\n",
      "18 ----> is\n",
      "11 ----> a\n",
      "1243 ----> multi\n",
      "419 ----> level\n",
      "420 ----> coordination\n",
      "201 ----> centre\n",
      "659 ----> designed\n",
      "8 ----> to\n",
      "65 ----> provide\n",
      "1244 ----> inter-agency\n",
      "420 ----> coordination\n",
      "7 ----> and\n",
      "660 ----> alert\n",
      "1 ----> the\n",
      "46 ----> people\n",
      "137 ----> about\n",
      "1 ----> the\n",
      "421 ----> threat\n",
      "6 ----> of\n",
      "1245 ----> emergency\n",
      "1246 ----> situations\n",
      "2 ----> .\n",
      "4 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[-10])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "integral-assignment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87, 92)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_targ, max_length_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "linear-vessel",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "dried-aerospace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2061, 2155, 320, 5)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_inp_size, vocab_tar_size, BUFFER_SIZE, steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "pleasant-treasurer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 92]), TensorShape([64, 87]))"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "daily-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "vulnerable-robinson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 92, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "younger-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "minute-clock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 92, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "traditional-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "lesbian-pointer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 2155)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "happy-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "personal-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "electric-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "radical-singer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.8900\n",
      "Epoch 1 Loss 1.9228\n",
      "Time taken for 1 epoch 167.7917618751526 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.7170\n",
      "Epoch 2 Loss 1.6323\n",
      "Time taken for 1 epoch 62.44920325279236 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.6377\n",
      "Epoch 3 Loss 1.6366\n",
      "Time taken for 1 epoch 61.812910318374634 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.5378\n",
      "Epoch 4 Loss 1.5720\n",
      "Time taken for 1 epoch 63.83378887176514 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.5536\n",
      "Epoch 5 Loss 1.5192\n",
      "Time taken for 1 epoch 61.66123414039612 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 1.6001\n",
      "Epoch 6 Loss 1.5153\n",
      "Time taken for 1 epoch 63.82371282577515 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 1.3534\n",
      "Epoch 7 Loss 1.4966\n",
      "Time taken for 1 epoch 61.68086123466492 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 1.4945\n",
      "Epoch 8 Loss 1.4836\n",
      "Time taken for 1 epoch 63.53247284889221 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 1.3786\n",
      "Epoch 9 Loss 1.4654\n",
      "Time taken for 1 epoch 61.53905248641968 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 1.4645\n",
      "Epoch 10 Loss 1.4443\n",
      "Time taken for 1 epoch 62.21631383895874 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                       batch,\n",
    "                                                       batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "treated-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot\n",
    "\n",
    "\n",
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "illegal-usage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6fee216650>"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "hydraulic-philippines",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<start> commissioners of east , north and south municipal corporations of delhi; <end>',\n",
       " '<start> दिल्ली के पूर्वी , उत्तरी और दक्षिणी नगर निगमों के आयुक्त; <end>')"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en[-10], sp[-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "cooked-thesis",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> <start> इस तरह आरएएन का कामकाज स्वास्थ्य एवं परिवार कल्याण विभाग के अधीन लाया जाएगा । <end> <end>\n",
      "Predicted translation: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/text/lib/python3.7/site-packages/ipykernel_launcher.py:50: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "/anaconda/envs/text/lib/python3.7/site-packages/ipykernel_launcher.py:51: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "/anaconda/envs/text/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 2310 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/anaconda/envs/text/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 2341 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/anaconda/envs/text/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 2339 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/anaconda/envs/text/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:238: RuntimeWarning: Glyph 2309 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/anaconda/envs/text/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 2310 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/anaconda/envs/text/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 2341 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/anaconda/envs/text/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 2339 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/anaconda/envs/text/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:201: RuntimeWarning: Glyph 2309 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sent =  '<start> पर्यटन से संबंधित सूचनाओं और जानकारियों (डेटा) का आदान प्रदान । <end>'\n",
    "translate(sp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-engagement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-pulse",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
