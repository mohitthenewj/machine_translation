{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "freelance-samoa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import pickle\n",
    "from inltk.inltk import tokenize\n",
    "from time import time\n",
    "\n",
    "from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "from torchtext.datasets import TranslationDataset\n",
    "from torchtext.data import Field, BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exact-partner",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "str_punct = '''[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~।]'''\n",
    "\n",
    "def tokenize_hi(text):\n",
    "    text = re.sub(str_punct,'',text).lower()\n",
    "    return [tok.lower() for tok in tokenize(text, \"hi\")]\n",
    "\n",
    "\n",
    "def tokenize_eng(text):\n",
    "    text = re.sub(str_punct,'',text).lower()\n",
    "    return [tok for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "hindi = Field(tokenize=tokenize_hi, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "english = Field(\n",
    "    tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\"\n",
    ")\n",
    "\n",
    "hindi = data.Field(tokenize=tokenize_hi)\n",
    "english = data.Field(tokenize=tokenize_eng)\n",
    "mt_train = datasets.TranslationDataset(\n",
    "     path='./data_torch/data_sm', exts=('.hi', '.en'),\n",
    "     fields=(hindi, english))\n",
    "\n",
    "hindi.build_vocab(mt_train, max_size=15000, min_freq=2)\n",
    "english.build_vocab(mt_train, max_size=15000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "local-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "invalid-hartford",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An advance is placed with the Medical Superint...</td>\n",
       "      <td>अग्रिम धन राशि इन अस्पतालों को चिकित्सा निरीक्...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since the DoHFW provides funds to the hospital...</td>\n",
       "      <td>चूंकि स्वास्थ्य एवं परिवार कल्याण विभाग अस्पता...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RAN functions can, therefore, be vested in DoHFW.</td>\n",
       "      <td>इस तरह आरएएन का कामकाज स्वास्थ्य एवं परिवार कल...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Managing Committee of RAN Society will meet to...</td>\n",
       "      <td>आरएएन, सोसायटी की प्रबंध समिति सोसायटी पंजीकरण...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In addition to this, Health Minister’s Cancer ...</td>\n",
       "      <td>इसके अलावा स्वास्थ्य मंत्री के कैंसर रोगी निधि...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56826</th>\n",
       "      <td>Principal Commissioner, DDA as Member Secretary.</td>\n",
       "      <td>प्रधान आयुक्‍त, डीडीए सदस्‍य सचिव के रूप में।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56827</th>\n",
       "      <td>The Committee constituted will submit its repo...</td>\n",
       "      <td>गठित की गई यह समिति 90 दिनों में अपनी रिपोर्ट ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56828</th>\n",
       "      <td>On submission of this report by the Committee,...</td>\n",
       "      <td>समिति द्वारा इस रिपोर्ट को सौंपे जाने पर, कैबि...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56829</th>\n",
       "      <td>The recommendations of the Committee will prov...</td>\n",
       "      <td>समिति की सिफारिशें दिल्‍ली की अनाधिकृत कालोनिय...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56830</th>\n",
       "      <td>This is for the first time that the conferment...</td>\n",
       "      <td>यह पहला मौका है जब दिल्‍ली की अनाधिकृत कालोनिय...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56831 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        english_sentence  \\\n",
       "0      An advance is placed with the Medical Superint...   \n",
       "1      Since the DoHFW provides funds to the hospital...   \n",
       "2      RAN functions can, therefore, be vested in DoHFW.   \n",
       "3      Managing Committee of RAN Society will meet to...   \n",
       "4      In addition to this, Health Minister’s Cancer ...   \n",
       "...                                                  ...   \n",
       "56826   Principal Commissioner, DDA as Member Secretary.   \n",
       "56827  The Committee constituted will submit its repo...   \n",
       "56828  On submission of this report by the Committee,...   \n",
       "56829  The recommendations of the Committee will prov...   \n",
       "56830  This is for the first time that the conferment...   \n",
       "\n",
       "                                          hindi_sentence  \n",
       "0      अग्रिम धन राशि इन अस्पतालों को चिकित्सा निरीक्...  \n",
       "1      चूंकि स्वास्थ्य एवं परिवार कल्याण विभाग अस्पता...  \n",
       "2      इस तरह आरएएन का कामकाज स्वास्थ्य एवं परिवार कल...  \n",
       "3      आरएएन, सोसायटी की प्रबंध समिति सोसायटी पंजीकरण...  \n",
       "4      इसके अलावा स्वास्थ्य मंत्री के कैंसर रोगी निधि...  \n",
       "...                                                  ...  \n",
       "56826      प्रधान आयुक्‍त, डीडीए सदस्‍य सचिव के रूप में।  \n",
       "56827  गठित की गई यह समिति 90 दिनों में अपनी रिपोर्ट ...  \n",
       "56828  समिति द्वारा इस रिपोर्ट को सौंपे जाने पर, कैबि...  \n",
       "56829  समिति की सिफारिशें दिल्‍ली की अनाधिकृत कालोनिय...  \n",
       "56830  यह पहला मौका है जब दिल्‍ली की अनाधिकृत कालोनिय...  \n",
       "\n",
       "[56831 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data_torch/pmindia.v1.hi-en.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "young-webcam",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([['▁अग्रिम', '▁धन', '▁राशि', '▁इन', '▁अस्पतालों', '▁को', '▁चिकित्सा', '▁निरीक्षक', 'ों', '▁को', '▁दी', '▁जाएगी', '▁जो', '▁हर', '▁मामले', '▁को', '▁देखते', '▁हुए', '▁सहायता', '▁प्रदान', '▁करेंगे'], [an, advance, is, placed, with, the, medical, superintendents, of, such, hospitals, who, then, provide, assistance, on, a, case, to, case, basis]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_train[0].__dict__.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unlike-effects",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'चूंकि स्वास्थ्य एवं परिवार कल्याण विभाग अस्पतालों को धनराशि प्रदान करता है इसलिए विभाग द्वारा अस्पतालों को सीधे अनुदान दिया जा सकता है।'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.hindi_sentence[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "over qualified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-facial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-anatomy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-layout",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-confusion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "threatened-making",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁चूंकि',\n",
       " '▁स्वास्थ्य',\n",
       " '▁एवं',\n",
       " '▁परिवार',\n",
       " '▁कल्याण',\n",
       " '▁विभाग',\n",
       " '▁अस्पतालों',\n",
       " '▁को',\n",
       " '▁धनराशि',\n",
       " '▁प्रदान',\n",
       " '▁करता',\n",
       " '▁है',\n",
       " '▁इसलिए',\n",
       " '▁विभाग',\n",
       " '▁द्वारा',\n",
       " '▁अस्पतालों',\n",
       " '▁को',\n",
       " '▁सीधे',\n",
       " '▁अनुदान',\n",
       " '▁दिया',\n",
       " '▁जा',\n",
       " '▁सकता',\n",
       " '▁है']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_hi(df.hindi_sentence[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "young-weather",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[since,\n",
       " the,\n",
       " dohfw,\n",
       " provides,\n",
       " funds,\n",
       " to,\n",
       " the,\n",
       " hospitals,\n",
       " the,\n",
       " grants,\n",
       " can,\n",
       " be,\n",
       " given,\n",
       " from,\n",
       " the,\n",
       " department,\n",
       " to,\n",
       " the,\n",
       " hospital,\n",
       " directly]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_eng(df.english_sentence[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-temperature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('hindi_vocab.pickle', 'wb') as handle:\n",
    "#     pickle.dump(hindi, handle)\n",
    "\n",
    "# with open('english_vocab.pickle', 'wb') as handle:\n",
    "#     pickle.dump(english, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_len,\n",
    "        device,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_word_embedding = nn.Embedding(src_vocab_size, embedding_size)\n",
    "        self.src_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "        self.trg_word_embedding = nn.Embedding(trg_vocab_size, embedding_size)\n",
    "        self.trg_position_embedding = nn.Embedding(max_len, embedding_size)\n",
    "\n",
    "        self.device = device\n",
    "        self.transformer = nn.Transformer(\n",
    "            embedding_size,\n",
    "            num_heads,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embedding_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
    "\n",
    "        # (N, src_len)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_seq_length, N = src.shape\n",
    "        trg_seq_length, N = trg.shape\n",
    "\n",
    "        src_positions = (\n",
    "            torch.arange(0, src_seq_length)\n",
    "            .unsqueeze(1)\n",
    "            .expand(src_seq_length, N)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        trg_positions = (\n",
    "            torch.arange(0, trg_seq_length)\n",
    "            .unsqueeze(1)\n",
    "            .expand(trg_seq_length, N)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        embed_src = self.dropout(\n",
    "            (self.src_word_embedding(src) + self.src_position_embedding(src_positions))\n",
    "        )\n",
    "        embed_trg = self.dropout(\n",
    "            (self.trg_word_embedding(trg) + self.trg_position_embedding(trg_positions))\n",
    "        )\n",
    "\n",
    "        src_padding_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "        out = self.transformer(\n",
    "            embed_src,\n",
    "            embed_trg,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_mask=trg_mask,\n",
    "        )\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're ready to define everything we need for training our Seq2Seq model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "load_model = False\n",
    "save_model = True\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 1000\n",
    "learning_rate = 3e-4\n",
    "batch_size = 32\n",
    "\n",
    "# Model hyperparameters\n",
    "src_vocab_size = len(hindi.vocab)\n",
    "trg_vocab_size = len(english.vocab)\n",
    "embedding_size = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dropout = 0.10\n",
    "max_len = 100\n",
    "forward_expansion = 4\n",
    "src_pad_idx = english.vocab.stoi[\"<pad>\"]\n",
    "\n",
    "# Tensorboard to get nice loss plot\n",
    "writer = SummaryWriter(\"runs/loss_plot\")\n",
    "step = 0\n",
    "\n",
    "print(f'length of src_vocab_size is {src_vocab_size}')\n",
    "\n",
    "print(f'length of input_size_decoder is {trg_vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-alignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = data.BucketIterator(\n",
    "     dataset=mt_train, batch_size=batch_size,\n",
    "     sort_key=lambda x: data.interleave_keys(len(x.src), len(x.trg)), device=device)\n",
    "\n",
    "model = Transformer(\n",
    "    embedding_size,\n",
    "    src_vocab_size,\n",
    "    trg_vocab_size,\n",
    "    src_pad_idx,\n",
    "    num_heads,\n",
    "    num_encoder_layers,\n",
    "    num_decoder_layers,\n",
    "    forward_expansion,\n",
    "    dropout,\n",
    "    max_len,\n",
    "    device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, factor=0.1, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "pad_idx = 1\n",
    "# english.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "sentence = \"प्रधानमंत्री ने कहा कि भारत में केंद्र सरकार बुनियादी ढांचे पर ध्यान केंद्रित कर रही है।\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    st = time()\n",
    "\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    if save_model:\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "        }\n",
    "        save_checkpoint(checkpoint)\n",
    "\n",
    "    model.eval()\n",
    "    translated_sentence = translate_sentence(\n",
    "        model, sentence, hindi, english, device, max_length=50\n",
    "    )\n",
    "\n",
    "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        # Get input and targets and get to cuda\n",
    "        inp_data = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(inp_data, target[:-1, :])\n",
    "\n",
    "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
    "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "        # way that we have output_words * batch_size that we want to send in into\n",
    "        # our cost function, so we need to do some reshapin.\n",
    "        # Let's also remove the start token while we're at it\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # plot to tensorboard\n",
    "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        step += 1\n",
    "    print(f'Total time taken for the epoch number {epoch} was {time() - st}')\n",
    "\n",
    "\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    scheduler.step(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-answer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-bracket",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-pollution",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-boundary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-shower",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################################################################################################################\n",
    "\n",
    "# import torch\n",
    "# import spacy\n",
    "# from torchtext.data.metrics import bleu_score\n",
    "# import sys\n",
    "\n",
    "\n",
    "# def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
    "#     # Load german tokenizer\n",
    "#     str_punct = '''[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~।]'''\n",
    "    \n",
    "#     # Create tokens using spacy and everything in lower case (which is what our vocab is)\n",
    "#     if type(sentence) == str:\n",
    "# #         tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
    "#         sentence = re.sub(str_punct,'',sentence).lower()\n",
    "#         tokens = [re.sub(str_punct,'',tok).lower() for tok in tokenize(sentence, \"hi\")]\n",
    "#     else:\n",
    "#         tokens = [token.lower() for token in sentence]\n",
    "\n",
    "#     # Add <SOS> and <EOS> in beginning and end respectively\n",
    "#     tokens.insert(0, german.init_token)\n",
    "#     tokens.append(german.eos_token)\n",
    "\n",
    "#     # Go through each german token and convert to an index\n",
    "#     text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "#     # Convert to Tensor\n",
    "#     sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "#     outputs = [english.vocab.stoi[\"<sos>\"]]\n",
    "#     for i in range(max_length):\n",
    "#         trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             output = model(sentence_tensor, trg_tensor)\n",
    "\n",
    "#         best_guess = output.argmax(2)[-1, :].item()\n",
    "#         outputs.append(best_guess)\n",
    "\n",
    "#         if best_guess == english.vocab.stoi[\"<eos>\"]:\n",
    "#             break\n",
    "\n",
    "#     translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n",
    "#     # remove start token\n",
    "#     return translated_sentence[1:]\n",
    "\n",
    "\n",
    "# def bleu(data, model, german, english, device):\n",
    "#     targets = []\n",
    "#     outputs = []\n",
    "\n",
    "#     for example in data:\n",
    "#         src = vars(example)[\"src\"]\n",
    "#         trg = vars(example)[\"trg\"]\n",
    "\n",
    "#         prediction = translate_sentence(model, src, german, english, device)\n",
    "#         prediction = prediction[:-1]  # remove <eos> token\n",
    "\n",
    "#         targets.append([trg])\n",
    "#         outputs.append(prediction)\n",
    "\n",
    "#     return bleu_score(outputs, targets)\n",
    "\n",
    "\n",
    "# def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "#     print(\"=> Saving checkpoint\")\n",
    "#     torch.save(state, filename)\n",
    "\n",
    "\n",
    "# def load_checkpoint(checkpoint, model, optimizer):\n",
    "#     print(\"=> Loading checkpoint\")\n",
    "#     model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "#     optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "# ################################################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
