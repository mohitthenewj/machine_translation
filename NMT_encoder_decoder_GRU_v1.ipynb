{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "quiet-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unlike-steal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/disk_ext/nlp\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "peripheral-furniture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==1.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "headed-battery",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "upset-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf.keras.models import Model  # This does not work!\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Embedding\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "handed-enhancement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.1.0', '2.2.4-tf')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__, tf.keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "noble-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "whole-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_text/pmindia.v1.hi-en.tsv', sep = '\\t', header =0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "religious-company",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56830, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "limited-third",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>An advance is placed with the Medical Superintendents of such hospitals who then provide assistance on a case to case basis.</th>\n",
       "      <th>अग्रिम धन राशि इन अस्पतालों को चिकित्सा निरीक्षकों को दी जाएगी, जो हर मामले को देखते हुए सहायता प्रदान करेंगे।</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Since the DoHFW provides funds to the hospital...</td>\n",
       "      <td>चूंकि स्वास्थ्य एवं परिवार कल्याण विभाग अस्पता...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RAN functions can, therefore, be vested in DoHFW.</td>\n",
       "      <td>इस तरह आरएएन का कामकाज स्वास्थ्य एवं परिवार कल...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Managing Committee of RAN Society will meet to...</td>\n",
       "      <td>आरएएन, सोसायटी की प्रबंध समिति सोसायटी पंजीकरण...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In addition to this, Health Minister’s Cancer ...</td>\n",
       "      <td>इसके अलावा स्वास्थ्य मंत्री के कैंसर रोगी निधि...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The timeline required for this is one year.</td>\n",
       "      <td>इसके लिए एक वर्ष का समय रखा गया है।</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  An advance is placed with the Medical Superintendents of such hospitals who then provide assistance on a case to case basis.  \\\n",
       "0  Since the DoHFW provides funds to the hospital...                                                                             \n",
       "1  RAN functions can, therefore, be vested in DoHFW.                                                                             \n",
       "2  Managing Committee of RAN Society will meet to...                                                                             \n",
       "3  In addition to this, Health Minister’s Cancer ...                                                                             \n",
       "4        The timeline required for this is one year.                                                                             \n",
       "\n",
       "  अग्रिम धन राशि इन अस्पतालों को चिकित्सा निरीक्षकों को दी जाएगी, जो हर मामले को देखते हुए सहायता प्रदान करेंगे।  \n",
       "0  चूंकि स्वास्थ्य एवं परिवार कल्याण विभाग अस्पता...                                                              \n",
       "1  इस तरह आरएएन का कामकाज स्वास्थ्य एवं परिवार कल...                                                              \n",
       "2  आरएएन, सोसायटी की प्रबंध समिति सोसायटी पंजीकरण...                                                              \n",
       "3  इसके अलावा स्वास्थ्य मंत्री के कैंसर रोगी निधि...                                                              \n",
       "4                इसके लिए एक वर्ष का समय रखा गया है।                                                              "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "interpreted-tribune",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "An advance is placed with the Medical Superintendents of such hospitals who then provide assistance on a case to case basis.    Since the DoHFW provides funds to the hospital...\n",
       "अग्रिम धन राशि इन अस्पतालों को चिकित्सा निरीक्षकों को दी जाएगी, जो हर मामले को देखते हुए सहायता प्रदान करेंगे।                  चूंकि स्वास्थ्य एवं परिवार कल्याण विभाग अस्पता...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "significant-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_src = df[df.columns[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "competent-equipment",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56830/56830 [00:00<00:00, 1152572.16it/s]\n"
     ]
    }
   ],
   "source": [
    "mark_start = 'ssss '\n",
    "mark_end = ' eeee'\n",
    "data_dest = []\n",
    "for line in tqdm(df[df.columns[1]]):\n",
    "    output_sentence =  mark_start +  line + mark_end\n",
    "    data_dest.append(output_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-saturday",
   "metadata": {},
   "source": [
    "### Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "identical-cylinder",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "collected-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerWrap(Tokenizer):\n",
    "    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, padding,\n",
    "                 reverse=False, num_words=None):\n",
    "        \"\"\"\n",
    "        :param texts: List of strings. This is the data-set.\n",
    "        :param padding: Either 'post' or 'pre' padding.\n",
    "        :param reverse: Boolean whether to reverse token-lists.\n",
    "        :param num_words: Max number of words to use.\n",
    "        \"\"\"\n",
    "\n",
    "        Tokenizer.__init__(self, num_words=num_words)\n",
    "\n",
    "        # Create the vocabulary from the texts.\n",
    "        self.fit_on_texts(texts)\n",
    "\n",
    "        # Create inverse lookup from integer-tokens to words.\n",
    "        self.index_to_word = dict(zip(self.word_index.values(),\n",
    "                                      self.word_index.keys()))\n",
    "\n",
    "        # Convert all texts to lists of integer-tokens.\n",
    "        # Note that the sequences may have different lengths.\n",
    "        self.tokens = self.texts_to_sequences(texts)\n",
    "\n",
    "        if reverse:\n",
    "            # Reverse the token-sequences.\n",
    "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
    "        \n",
    "            # Sequences that are too long should now be truncated\n",
    "            # at the beginning, which corresponds to the end of\n",
    "            # the original sequences.\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            # Sequences that are too long should be truncated\n",
    "            # at the end.\n",
    "            truncating = 'post'\n",
    "\n",
    "        # The number of integer-tokens in each sequence.\n",
    "        self.num_tokens = [len(x) for x in self.tokens]\n",
    "\n",
    "        # Max number of tokens to use in all sequences.\n",
    "        # We will pad / truncate all sequences to this length.\n",
    "        # This is a compromise so we save a lot of memory and\n",
    "        # only have to truncate maybe 5% of all the sequences.\n",
    "        self.max_tokens = np.mean(self.num_tokens) \\\n",
    "                          + 2 * np.std(self.num_tokens)\n",
    "        self.max_tokens = int(self.max_tokens)\n",
    "\n",
    "        # Pad / truncate all token-sequences to the given length.\n",
    "        # This creates a 2-dim numpy matrix that is easier to use.\n",
    "        self.tokens_padded = pad_sequences(self.tokens,\n",
    "                                           maxlen=self.max_tokens,\n",
    "                                           padding=padding,\n",
    "                                           truncating=truncating)\n",
    "\n",
    "    def token_to_word(self, token):\n",
    "        \"\"\"Lookup a single word from an integer-token.\"\"\"\n",
    "\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\n",
    "        return word \n",
    "\n",
    "    def tokens_to_string(self, tokens):\n",
    "        \"\"\"Convert a list of integer-tokens to a string.\"\"\"\n",
    "\n",
    "        # Create a list of the individual words.\n",
    "        words = [self.index_to_word[token]\n",
    "                 for token in tokens\n",
    "                 if token != 0]\n",
    "        \n",
    "        # Concatenate the words to a single string\n",
    "        # with space between all the words.\n",
    "        text = \" \".join(words)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def text_to_tokens(self, text, reverse=False, padding=False):\n",
    "        \"\"\"\n",
    "        Convert a single text-string to tokens with optional\n",
    "        reversal and padding.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert to tokens. Note that we assume there is only\n",
    "        # a single text-string so we wrap it in a list.\n",
    "        tokens = self.texts_to_sequences([text])\n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        if reverse:\n",
    "            # Reverse the tokens.\n",
    "            tokens = np.flip(tokens, axis=1)\n",
    "\n",
    "            # Sequences that are too long should now be truncated\n",
    "            # at the beginning, which corresponds to the end of\n",
    "            # the original sequences.\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            # Sequences that are too long should be truncated\n",
    "            # at the end.\n",
    "            truncating = 'post'\n",
    "\n",
    "        if padding:\n",
    "            # Pad and truncate sequences to the given length.\n",
    "            tokens = pad_sequences(tokens,\n",
    "                                   maxlen=self.max_tokens,\n",
    "                                   padding='pre',\n",
    "                                   truncating=truncating)\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "buried-poland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.46 s, sys: 29.1 ms, total: 2.48 s\n",
      "Wall time: 2.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer_src = TokenizerWrap(texts=data_src,\n",
    "                              padding='pre',\n",
    "                              reverse=True,\n",
    "                              num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "hundred-nightlife",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.58 s, sys: 25.7 ms, total: 3.61 s\n",
      "Wall time: 3.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer_dest = TokenizerWrap(texts=data_dest,\n",
    "                               padding='post',\n",
    "                               reverse=False,\n",
    "                               num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hired-simpson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56830, 38)\n",
      "(56830, 43)\n"
     ]
    }
   ],
   "source": [
    "tokens_src = tokenizer_src.tokens_padded\n",
    "tokens_dest = tokenizer_dest.tokens_padded\n",
    "print(tokens_src.shape)\n",
    "print(tokens_dest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "angry-zimbabwe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_start = tokenizer_dest.word_index[mark_start.strip()]\n",
    "token_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "selected-marine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sra 1860 act registration societies of provisions per as ab body autonomous the dissolve to meet will society ran of committee managing',\n",
       " 'ssss आरएएन सोसायटी की प्रबंध समिति सोसायटी पंजीकरण अधिनियम 1860 के प्रावधानों के तहत स्वायत्तशासी निकायों को रद्द करने के लिए बैठक करेगा। eeee')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_src.tokens_to_string(tokens_src[2]), tokenizer_dest.tokens_to_string(tokens_dest[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sacred-affiliate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_dest[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "surprised-democracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((56830, 42), (56830, 42))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data = tokens_src\n",
    "decoder_input_data = tokens_dest[:, :-1]\n",
    "decoder_output_data = tokens_dest[:, 1:]\n",
    "decoder_output_data.shape, decoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-horizon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bored-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(None, ), name='encoder_input')\n",
    "\n",
    "embedding_size = 128\n",
    "encoder_embedding = Embedding(input_dim=num_words,\n",
    "                              output_dim=embedding_size,\n",
    "                              name='encoder_embedding')\n",
    "state_size = 512\n",
    "encoder_gru1 = GRU(state_size, name='encoder_gru1',\n",
    "                   return_sequences=True)\n",
    "encoder_gru2 = GRU(state_size, name='encoder_gru2',\n",
    "                   return_sequences=True)\n",
    "encoder_gru3 = GRU(state_size, name='encoder_gru3',\n",
    "                   return_sequences=False)\n",
    "\n",
    "def connect_encoder():\n",
    "    # Start the neural network with its input-layer.\n",
    "    net = encoder_input\n",
    "    \n",
    "    # Connect the embedding-layer.\n",
    "    net = encoder_embedding(net)\n",
    "\n",
    "    # Connect all the GRU-layers.\n",
    "    net = encoder_gru1(net)\n",
    "    net = encoder_gru2(net)\n",
    "    net = encoder_gru3(net)\n",
    "\n",
    "    # This is the output of the encoder.\n",
    "    encoder_output = net\n",
    "    \n",
    "    return encoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "velvet-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = connect_encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-array",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "nuclear-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decoder_initial_state = Input(shape=(state_size,),\n",
    "                              name='decoder_initial_state')\n",
    "decoder_input = Input(shape=(None, ), name='decoder_input')\n",
    "\n",
    "decoder_embedding = Embedding(input_dim=num_words,\n",
    "                              output_dim=embedding_size,\n",
    "                              name='decoder_embedding')\n",
    "\n",
    "decoder_gru1 = GRU(state_size, name='decoder_gru1',\n",
    "                   return_sequences=True)\n",
    "decoder_gru2 = GRU(state_size, name='decoder_gru2',\n",
    "                   return_sequences=True)\n",
    "decoder_gru3 = GRU(state_size, name='decoder_gru3',\n",
    "                   return_sequences=True)\n",
    "\n",
    "decoder_dense = Dense(num_words,\n",
    "                      activation='softmax',\n",
    "                      name='decoder_output')\n",
    "\n",
    "def connect_decoder(initial_state):\n",
    "    # Start the decoder-network with its input-layer.\n",
    "    net = decoder_input\n",
    "\n",
    "    # Connect the embedding-layer.\n",
    "    net = decoder_embedding(net)\n",
    "    \n",
    "    # Connect all the GRU-layers.\n",
    "    net = decoder_gru1(net, initial_state=initial_state)\n",
    "    net = decoder_gru2(net, initial_state=initial_state)\n",
    "    net = decoder_gru3(net, initial_state=initial_state)\n",
    "\n",
    "    # Connect the final dense layer that converts to\n",
    "    # one-hot encoded arrays.\n",
    "    decoder_output = decoder_dense(net)\n",
    "    \n",
    "    return decoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "suspected-language",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = connect_decoder(initial_state=encoder_output)\n",
    "\n",
    "model_train = Model(inputs=[encoder_input, decoder_input],\n",
    "                    outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "north-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_encoder = Model(inputs=[encoder_input],\n",
    "                      outputs=[encoder_output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "existing-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = connect_decoder(initial_state=decoder_initial_state)\n",
    "\n",
    "model_decoder = Model(inputs=[decoder_input, decoder_initial_state],\n",
    "                      outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "pointed-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train.compile(optimizer=RMSprop(lr=1e-3),\n",
    "                    loss='sparse_categorical_crossentropy')\n",
    "\n",
    "path_checkpoint = './checkpoint.keras'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)\n",
    "\n",
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=3, verbose=1)\n",
    "\n",
    "callback_tensorboard = TensorBoard(log_dir='./logs/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False)\n",
    "\n",
    "callbacks = [callback_early_stopping,\n",
    "             callback_checkpoint,\n",
    "             callback_tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "proved-wesley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error trying to load checkpoint.\n",
      "Shapes (40000, 128) and (20000, 128) are incompatible\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model_train.load_weights(path_checkpoint)\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "spread-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "x_data = \\\n",
    "{\n",
    "    'encoder_input': encoder_input_data,\n",
    "    'decoder_input': decoder_input_data\n",
    "}\n",
    "\n",
    "y_data = \\\n",
    "{\n",
    "    'decoder_output': decoder_output_data\n",
    "}\n",
    "\n",
    "validation_split = 10000 / len(encoder_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-edwards",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 46830 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "46464/46830 [============================>.] - ETA: 13s - loss: 4.0557\n",
      "Epoch 00001: val_loss improved from inf to 3.52699, saving model to ./checkpoint.keras\n",
      "46830/46830 [==============================] - 1909s 41ms/sample - loss: 4.0518 - val_loss: 3.5270\n",
      "Epoch 2/10\n",
      "44160/46830 [===========================>..] - ETA: 1:47 - loss: 3.3143\n",
      "Epoch 00002: val_loss improved from 3.52699 to 3.22249, saving model to ./checkpoint.keras\n",
      "46830/46830 [==============================] - 2001s 43ms/sample - loss: 3.3041 - val_loss: 3.2225\n",
      "Epoch 3/10\n",
      "46464/46830 [============================>.] - ETA: 13s - loss: 3.0783\n",
      "Epoch 00003: val_loss improved from 3.22249 to 2.99853, saving model to ./checkpoint.keras\n",
      "46830/46830 [==============================] - 1938s 41ms/sample - loss: 3.0763 - val_loss: 2.9985\n",
      "Epoch 4/10\n",
      "46464/46830 [============================>.] - ETA: 14s - loss: 2.8351\n",
      "Epoch 00004: val_loss improved from 2.99853 to 2.86712, saving model to ./checkpoint.keras\n",
      "46830/46830 [==============================] - 1978s 42ms/sample - loss: 2.8342 - val_loss: 2.8671\n",
      "Epoch 5/10\n",
      "46464/46830 [============================>.] - ETA: 14s - loss: 2.6826\n",
      "Epoch 00005: val_loss improved from 2.86712 to 2.71536, saving model to ./checkpoint.keras\n",
      "46830/46830 [==============================] - 1975s 42ms/sample - loss: 2.6822 - val_loss: 2.7154\n",
      "Epoch 6/10\n",
      "19584/46830 [===========>..................] - ETA: 17:55 - loss: 2.5291"
     ]
    }
   ],
   "source": [
    "model_train.fit(x=x_data,\n",
    "                y=y_data,\n",
    "                batch_size=384,\n",
    "                epochs=10,\n",
    "                validation_split=validation_split,\n",
    "                callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "dress-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_text, true_output_text=None):\n",
    "    \"\"\"Translate a single text-string.\"\"\"\n",
    "\n",
    "    # Convert the input-text to integer-tokens.\n",
    "    # Note the sequence of tokens has to be reversed.\n",
    "    # Padding is probably not necessary.\n",
    "    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n",
    "                                                reverse=True,\n",
    "                                                padding=True)\n",
    "    \n",
    "    # Get the output of the encoder's GRU which will be\n",
    "    # used as the initial state in the decoder's GRU.\n",
    "    # This could also have been the encoder's final state\n",
    "    # but that is really only necessary if the encoder\n",
    "    # and decoder use the LSTM instead of GRU because\n",
    "    # the LSTM has two internal states.\n",
    "    initial_state = model_encoder.predict(input_tokens)\n",
    "\n",
    "    # Max number of tokens / words in the output sequence.\n",
    "    max_tokens = tokenizer_dest.max_tokens\n",
    "\n",
    "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
    "    # This holds just a single sequence of integer-tokens,\n",
    "    # but the decoder-model expects a batch of sequences.\n",
    "    shape = (1, max_tokens)\n",
    "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
    "\n",
    "    # The first input-token is the special start-token for 'ssss '.\n",
    "    token_int = token_start\n",
    "\n",
    "    # Initialize an empty output-text.\n",
    "    output_text = ''\n",
    "\n",
    "    # Initialize the number of tokens we have processed.\n",
    "    count_tokens = 0\n",
    "\n",
    "    # While we haven't sampled the special end-token for ' eeee'\n",
    "    # and we haven't processed the max number of tokens.\n",
    "    while token_int != token_end and count_tokens < max_tokens:\n",
    "        # Update the input-sequence to the decoder\n",
    "        # with the last token that was sampled.\n",
    "        # In the first iteration this will set the\n",
    "        # first element to the start-token.\n",
    "        decoder_input_data[0, count_tokens] = token_int\n",
    "\n",
    "        # Wrap the input-data in a dict for clarity and safety,\n",
    "        # so we are sure we input the data in the right order.\n",
    "        x_data = \\\n",
    "        {\n",
    "            'decoder_initial_state': initial_state,\n",
    "            'decoder_input': decoder_input_data\n",
    "        }\n",
    "\n",
    "        # Note that we input the entire sequence of tokens\n",
    "        # to the decoder. This wastes a lot of computation\n",
    "        # because we are only interested in the last input\n",
    "        # and output. We could modify the code to return\n",
    "        # the GRU-states when calling predict() and then\n",
    "        # feeding these GRU-states as well the next time\n",
    "        # we call predict(), but it would make the code\n",
    "        # much more complicated.\n",
    "\n",
    "        # Input this data to the decoder and get the predicted output.\n",
    "        decoder_output = model_decoder.predict(x_data)\n",
    "\n",
    "        # Get the last predicted token as a one-hot encoded array.\n",
    "        token_onehot = decoder_output[0, count_tokens, :]\n",
    "        \n",
    "        # Convert to an integer-token.\n",
    "        token_int = np.argmax(token_onehot)\n",
    "\n",
    "        # Lookup the word corresponding to this integer-token.\n",
    "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
    "\n",
    "        # Append the word to the output-text.\n",
    "        output_text += \" \" + sampled_word\n",
    "\n",
    "        # Increment the token-counter.\n",
    "        count_tokens += 1\n",
    "\n",
    "    # Sequence of tokens output by the decoder.\n",
    "    output_tokens = decoder_input_data[0]\n",
    "    \n",
    "    # Print the input-text.\n",
    "    print(\"Input text:\")\n",
    "    print(input_text)\n",
    "    print()\n",
    "\n",
    "    # Print the translated output-text.\n",
    "    print(\"Translated text:\")\n",
    "    print(output_text)\n",
    "    print()\n",
    "\n",
    "    # Optionally print the true translated text.\n",
    "    if true_output_text is not None:\n",
    "        print(\"True output text:\")\n",
    "        print(true_output_text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "critical-cutting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ssss इसके अलावा स्वास्थ्य मंत्री के कैंसर रोगी निधि को भी विभाग को स्थानांतरित कर दिया जाएगा। eeee'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dest[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "prescription-strategy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_end = tokenizer_dest.word_index[mark_end.strip()]\n",
    "token_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "structured-topic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "In addition to this, Health Minister’s Cancer Patient Fund (HMCPF) shall also be transferred to the Department.\n",
      "\n",
      "Translated text:\n",
      " इस परियोजना के तहत इस प्रकार की अवधि में भी कमी आएगी। eeee\n",
      "\n",
      "True output text:\n",
      "ssss इसके अलावा स्वास्थ्य मंत्री के कैंसर रोगी निधि को भी विभाग को स्थानांतरित कर दिया जाएगा। eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 3\n",
    "translate(input_text=data_src[idx],\n",
    "          true_output_text=data_dest[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "average-yorkshire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "He said the Speaker’s name itself suggests that everyone will feel she is a friend.\n",
      "\n",
      "Translated text:\n",
      " उन्‍होंने कहा कि यह एक बार फिर है जो भारत के लिए भी प्रेरणा नहीं है। eeee\n",
      "\n",
      "True output text:\n",
      "ssss उन्‍होंने कहा कि अध्‍यक्ष का नाम ही सभी को उनका मित्र होने का एहसास कराता है। eeee\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 22\n",
    "translate(input_text=data_src[idx],\n",
    "          true_output_text=data_dest[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-rebel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
