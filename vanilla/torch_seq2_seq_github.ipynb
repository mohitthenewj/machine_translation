{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "norman-vancouver",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adjacent-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_ger = spacy.load(\"de\")\n",
    "spacy_eng = spacy.load(\"en\")\n",
    "\n",
    "\n",
    "def tokenize_ger(text):\n",
    "    return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
    "\n",
    "def tokenize_eng(text):\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "\n",
    "german = Field(tokenize=tokenize_ger, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "\n",
    "english = Field(\n",
    "    tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\"\n",
    ")\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(\n",
    "    exts=(\".de\", \".en\"), fields=(german, english)\n",
    ")\n",
    "\n",
    "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "english.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N) where N is batch size\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape: (seq_length, N, hidden_size)\n",
    "\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
    "        # is 1 here because we are sending in a single word and not a sentence\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # outputs shape: (1, N, hidden_size)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "\n",
    "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
    "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
    "        # just gonna remove the first dim\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(english.vocab)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # Grab the first input to the Decoder which will be <SOS> token\n",
    "        x = target[0]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Use previous hidden, cell as context from encoder at start\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "\n",
    "            # Store next output prediction\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # With probability of teacher_force_ratio we take the actual next word\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            # Teacher Forcing is used so that the model gets used to seeing\n",
    "            # similar inputs at training and testing time, if teacher forcing is 1\n",
    "            # then inputs at test time might be completely different than what the\n",
    "            # network is used to. This was a long comment.\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "### We're ready to define everything we need for training our Seq2Seq model ###\n",
    "\n",
    "# Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "# Model hyperparameters\n",
    "load_model = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size_encoder = len(german.vocab)\n",
    "input_size_decoder = len(english.vocab)\n",
    "output_size = len(english.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024  # Needs to be the same for both RNN's\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "# Tensorboard to get nice loss plot\n",
    "writer = SummaryWriter(f\"runs/loss_plot\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-committee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['ascends', 'paving', 'paving', 'passes', 'passes', 'infant', 'nintendo', 'dune', 'hatted', 'sleeping', 'el', 'skyline', 'jumping', 'irons', 'skyline', 'skyline', 'exit', 'hot', 'cameramen', 'meal', 'meal', 'juggles', 'juggles', 'juggles', 'bows', 'jeep', 'meal', 'juggles', 'juggles', 'juggles', 'jeep', 'jeep', 'paintball', 'paintball', 'sumo', 'knitted', 'jumping', 'jumping', 'irons', 'starts', 'skyline', 'skyline', 'exit', 'jumping', 'irons', 'skyline', 'hot', 'jumping', 'gesture', 'gesture']\n",
      "[Epoch 1 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'young', 'girl', 'in', 'a', 'blue', 'shirt', 'and', 'a', 'shirt', 'is', 'a', 'a', '.', '.', '<eos>']\n",
      "[Epoch 3 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'worker', 'with', 'a', '<unk>', 'is', 'is', 'a', 'a', 'a', 'a', 'a', 'a', '<unk>', '.', '<eos>']\n",
      "[Epoch 5 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'a', 'number', 'of', 'a', 'by', 'a', 'large', 'of', 'a', 'large', 'building', '.', '<eos>']\n",
      "[Epoch 6 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'skier', 'with', 'a', 'number', 'off', 'by', 'a', 'large', 'of', 'a', 'large', '.', '.', '<eos>']\n",
      "[Epoch 7 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'a', '<unk>', 'is', 'being', 'pulled', 'by', 'a', 'large', 'of', 'a', 'large', '.', '.', '<eos>']\n",
      "[Epoch 8 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', '<unk>', 'from', 'the', 'water', 'from', 'a', 'large', 'large', 'large', '.', '<eos>']\n",
      "[Epoch 9 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'with', 'a', 'pulled', 'by', 'by', 'a', 'large', 'boat', 'by', 'a', 'large', '.', '.', '<eos>']\n",
      "[Epoch 10 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', '<unk>', 'pulled', 'by', 'a', 'boat', 'by', 'a', 'large', 'surrounded', 'by', 'a', '.', '<eos>']\n",
      "[Epoch 11 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'a', 'pulled', 'pulled', 'by', 'by', 'a', 'large', ',', 'surrounded', 'by', '.', '<eos>']\n",
      "[Epoch 12 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'the', '<unk>', 'pulled', 'by', 'a', 'boat', 'from', 'a', 'large', 'cable', '.', '<eos>']\n",
      "[Epoch 13 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', '<unk>', 'pulled', 'pulled', 'by', 'a', 'boat', 'from', 'a', 'large', 'cable', '.', '<eos>']\n",
      "[Epoch 14 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'the', '<unk>', 'pulled', 'pulled', 'by', 'a', 'large', 'men', 'being', 'pulled', 'by', 'a', 'large', 'cruise', '.', '<eos>']\n",
      "[Epoch 15 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'two', 'men', 'pulled', 'pulled', 'by', 'a', 'large', 'boat', 'being', 'pulled', 'by', 'a', 'horses', '.', '<eos>']\n",
      "[Epoch 16 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', '<unk>', 'pulled', 'pulled', 'by', 'a', 'large', 'cable', 'from', 'a', 'large', 'cable', '.', '<eos>']\n",
      "[Epoch 17 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'by', 'horses', 'from', 'a', 'large', 'cable', '.', '<eos>']\n",
      "[Epoch 18 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'large', 'cable', 'from', 'a', 'large', 'cable', '.', '<eos>']\n",
      "[Epoch 19 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'being', 'pulled', 'pulled', 'by', 'a', 'large', 'cable', '.', 'horses', '.', '<eos>']\n",
      "[Epoch 20 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'bucket', 'of', 'a', 'large', 'cable', '.', '<eos>']\n",
      "[Epoch 21 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'a', 'large', 'cow', 'from', 'a', 'wooden', 'horses', '.', '<eos>']\n",
      "[Epoch 22 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'carrying', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'boat', 'from', 'a', 'large', 'cable', '.', '<eos>']\n",
      "[Epoch 23 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'carrying', 'several', 'men', 'lifted', 'from', 'from', 'a', 'boat', 'from', 'a', 'large', 'cable', '.', '<eos>']\n",
      "[Epoch 24 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'carrying', 'two', 'men', 'being', 'pulled', 'by', 'shore', 'from', 'a', 'large', 'cable', '.', '<eos>']\n",
      "[Epoch 25 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'carrying', 'two', 'men', 'being', 'pulled', 'by', 'a', 'large', 'boat', 'being', 'pulled', 'by', 'horses', '.', '<eos>']\n",
      "[Epoch 26 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'carrying', 'several', 'men', 'pulled', 'pulled', 'by', 'a', 'boat', 'by', 'a', 'large', 'cable', '.', '<eos>']\n",
      "[Epoch 27 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'with', 'several', 'men', 'being', 'pulled', 'by', 'a', 'large', 'team', 'of', 'horses', '.', 'horses', '.', '<eos>']\n",
      "[Epoch 28 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'carrying', 'several', 'men', 'lifted', 'pulled', 'by', 'a', 'painting', 'by', 'a', 'large', 'cable', '.', '<eos>']\n",
      "[Epoch 29 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'carrying', 'several', 'men', 'pulled', 'pulled', 'pulled', 'by', 'shore', 'in', 'a', 'large', 'horses', '.', '<eos>']\n",
      "[Epoch 30 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'carrying', 'several', 'men', 'pulled', 'pulled', 'by', 'shore', 'by', 'a', 'boat', ',', 'a', 'birds', '.', '<eos>']\n",
      "[Epoch 31 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'carrying', 'several', 'men', 'pulled', 'pulled', 'by', 'shore', 'of', 'a', 'large', 'cable', 'horses', 'horses', '.', '<eos>']\n",
      "[Epoch 32 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'carrying', 'several', 'men', 'being', 'pulled', 'by', 'shore', 'by', 'a', 'boat', 'from', 'horses', '.', '<eos>']\n",
      "[Epoch 33 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'carrying', 'several', 'men', 'being', 'pulled', 'by', 'shore', 'by', 'a', 'large', 'cable', '.', '<eos>']\n",
      "[Epoch 34 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'carrying', 'several', 'men', 'being', 'pulled', 'by', 'shore', 'by', 'a', 'boat', 'from', 'a', 'large', 'team', 'of', 'horses', '.', '<eos>']\n",
      "[Epoch 35 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'carrying', 'several', 'men', 'is', 'pulled', 'by', 'shore', 'by', 'a', 'large', 'team', 'team', 'horses', '.', '<eos>']\n",
      "[Epoch 36 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'carrying', 'several', 'men', 'being', 'pulled', 'by', 'men', 'by', 'a', 'boat', 'from', 'a', 'large', 'team', '.', '<eos>']\n",
      "[Epoch 37 / 100]\n",
      "=> Saving checkpoint\n",
      "Translated example sentence: \n",
      " ['a', 'boat', 'carrying', 'several', 'men', 'pulled', 'pulled', 'by', 'shore', 'of', 'a', 'large', 'cable', 'horses', 'horses', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "encoder_net = Encoder(\n",
    "    input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
    ").to(device)\n",
    "\n",
    "decoder_net = Decoder(\n",
    "    input_size_decoder,\n",
    "    decoder_embedding_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    num_layers,\n",
    "    dec_dropout,\n",
    ").to(device)\n",
    "\n",
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "\n",
    "sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    translated_sentence = translate_sentence(\n",
    "        model, sentence, german, english, device, max_length=50\n",
    "    )\n",
    "\n",
    "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        # Get input and targets and get to cuda\n",
    "        inp_data = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(inp_data, target)\n",
    "\n",
    "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
    "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "        # way that we have output_words * batch_size that we want to send in into\n",
    "        # our cost function, so we need to do some reshapin. While we're at it\n",
    "        # Let's also remove the start token while we're at it\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Plot to tensorboard\n",
    "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        step += 1\n",
    "\n",
    "\n",
    "score = bleu(test_data[1:100], model, german, english, device)\n",
    "print(f\"Bleu score {score*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-eclipse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-garbage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-trunk",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-claim",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-private",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-congress",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-meeting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "spiritual-auckland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu score 17.44\n"
     ]
    }
   ],
   "source": [
    "score = bleu(test_data[1:100], model, german, english, device)\n",
    "print(f\"Bleu score {score*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "freelance-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "translated_sentence = translate_sentence(\n",
    "    model, sentence, german, english, device, max_length=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "voluntary-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '''Wo bist du?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "alone-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_sentence = translate_sentence(\n",
    "    model, sentence, german, english, device, max_length=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "crazy-distribution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'workers', 'checking', 'workers', '<unk>', 'lap', '.', '<eos>']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fitting-electronics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a boat carrying several men is pulled to shore by a large team of horses . <eos>'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-capture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-lambda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "approved-celebration",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exposed-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_ger = spacy.load(\"de\")\n",
    "spacy_eng = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "painful-steam",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "headed-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ger(text):\n",
    "    return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
    "def tokenize_eng(text):\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "damaged-broadcasting",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/text/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "german = Field(tokenize=tokenize_ger, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "english = Field(\n",
    "    tokenize=tokenize_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "annual-spider",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/text/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = (\".de\", \".en\"), fields = (german, english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "theoretical-catalyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "german.build_vocab(train_data, max_size = 10000, min_freq = 2)\n",
    "english.build_vocab(train_data, max_size = 10000, min_freq = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "noble-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(nn.LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "monthly-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "    def forward(self, x):\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        \n",
    "        outputs, (hidden, cell) =  self.rnn(embedding)\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pregnant-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size,output_size, num_layers, p):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        \n",
    "        x= x.squeeze(0)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        \n",
    "        predictions = self.fc(outputs)\n",
    "        \n",
    "        predictions = predictions.squeeze(0)\n",
    "        \n",
    "        return predictions, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sitting-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        \n",
    "        target_vocab_size = len(english_vocab)\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(device)\n",
    "        x = target[0]\n",
    "        \n",
    "        \n",
    "        for t in range(target_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            \n",
    "            best_guess = output.argmax(1)\n",
    "            \n",
    "            x= target[t] if torch.random() < teacher_force_ratio else best_guess\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "monthly-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "# Model hyperparameters\n",
    "load_model = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size_encoder = len(german.vocab)\n",
    "input_size_decoder = len(english.vocab)\n",
    "output_size = len(english.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024  # Needs to be the same for both RNN's\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ceramic-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f\"runs/loss_plot\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "classified-assembly",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/text/lib/python3.7/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
      "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "diagnostic-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_net = Encoder(\n",
    "    input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "frozen-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_net = Decoder(\n",
    "    input_size_decoder,\n",
    "    decoder_embedding_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    num_layers,\n",
    "    dec_dropout,\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "convenient-forty",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "different-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = english.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "statutory-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "global-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "raised-graph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 / 1]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'hidden' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-34f245a361c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     translated_sentence = translate_sentence(\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgerman\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menglish\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Translated example sentence: \\n {translated_sentence}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/disk_ext/nlp/seq2seq_torch/utils.py\u001b[0m in \u001b[0;36mtranslate_sentence\u001b[0;34m(model, sentence, german, english, device, max_length)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Build encoder hidden, cell state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menglish\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<sos>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/text/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-33cba7438c09>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'hidden' referenced before assignment"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint)\n",
    "    model.eval()\n",
    "\n",
    "    translated_sentence = translate_sentence(\n",
    "        model, sentence, german, english, device, max_length=50\n",
    "    )\n",
    "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        # Get input and targets and get to cuda\n",
    "        inp_data = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(inp_data, target)\n",
    "        \n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parametrs(), max_norm = 1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        step += 1\n",
    "        \n",
    "score = bleu(test_data[1:100], model, german, english, device)\n",
    "print(f\"Bleu score {score*100:.2f}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-commissioner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
