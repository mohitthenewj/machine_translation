{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# raw_data = './data_torch/pmindia.v1.hi-en.csv'\n",
    "\n",
    "# df = pd.read_csv(raw_data)\n",
    "\n",
    "# str_punct = '''[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~।]'''\n",
    "# # remove very long sentences and sentences where translations are \n",
    "# # not of roughly equal length\n",
    "# df['eng_len'] = df['english_sentence'].str.count(' ')\n",
    "# df['hi_len'] = df['hindi_sentence'].str.count(' ')\n",
    "\n",
    "# # %matplotlib inline\n",
    "\n",
    "# # df.eng_len.plot()\n",
    "\n",
    "# # df.head()\n",
    "\n",
    "# # df.shape\n",
    "\n",
    "# df.query('hi_len < 50 & eng_len < 50').shape\n",
    "\n",
    "# df = df.query('hi_len < 50 & eng_len < 50')\n",
    "# df = df.query('hi_len < eng_len * 1.5 & hi_len * 1.5 > eng_len')\n",
    "# df['english_sentence'] = df['english_sentence'].apply(lambda x : re.sub(str_punct,'',x))\n",
    "# df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x : re.sub(str_punct,'',x))\n",
    "\n",
    "# # np.savetxt(r'./data_torch/data.en', df.english_sentence.head(30000), fmt='%s')\n",
    "# # np.savetxt(r'./data_torch/data.hi', df.hindi_sentence.head(30000), fmt='%s')\n",
    "\n",
    "# np.savetxt(r'./data_torch/data_sm.en', df.english_sentence.head(30), fmt='%s')\n",
    "# np.savetxt(r'./data_torch/data_sm.hi', df.hindi_sentence.head(30), fmt='%s')\n",
    "\n",
    "\n",
    "# # np.savetxt(r'./data_torch/data_val.en', df.english_sentence.tail(1000), fmt='%s')\n",
    "# # np.savetxt(r'./data_torch/data_val.hi', df.hindi_sentence.tail(1000), fmt='%s')\n",
    "\n",
    "# # df.sample(frac=0.1).shape\n",
    "# # df = df.sample(frac=0.01)\n",
    "\n",
    "# # from sklearn.model_selection import train_test_split\n",
    "# # # create train and validation set \n",
    "# # train, val = train_test_split(df[['english_sentence', 'hindi_sentence']], test_size=0.2)\n",
    "# # train.shape, val.shape\n",
    "# # train.to_csv(\"./data_torch/train.csv\", index=False)\n",
    "# # val.to_csv(\"./data_torch/val.csv\", index=False)\n",
    "\n",
    "# # train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint\n",
    "from inltk.inltk import tokenize\n",
    "# from inltk.inltk import setup\n",
    "# setup('hi')\n",
    "from torchtext import datasets\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy_ger = spacy.load(\"de_core_news_sm\")\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_hi(text):\n",
    "    return tokenize(text, \"hi\")\n",
    "\n",
    "def tokenize_eng(text):\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = data.Field(tokenize=tokenize_hi)\n",
    "trg = data.Field(tokenize=tokenize_eng)\n",
    "# mt_train = datasets.TranslationDataset(\n",
    "#      fields=(src, trg))\n",
    "\n",
    "mt_test = datasets.TranslationDataset(\n",
    "     path='./data_torch/data_val', exts=('.hi', '.en'),\n",
    "     fields=(src, trg))\n",
    "\n",
    "# src.build_vocab(mt_train, max_size=20000, min_freq=2)\n",
    "# trg.build_vocab(mt_train, max_size=20000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('src.pickle', 'rb') as handle:\n",
    "    src_loaded = pickle.load(handle)\n",
    "\n",
    "with open('trg.pickle', 'rb') as handle:\n",
    "    trg_loaded = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N) where N is batch size\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape: (seq_length, N, hidden_size)\n",
    "\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # outputs shape: (1, N, hidden_size)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(trg.vocab)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # Grab the first input to the Decoder which will be <SOS> token\n",
    "        x = target[0]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Use previous hidden, cell as context from encoder at start\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "\n",
    "            outputs[t] = output\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "# Model hyperparameters\n",
    "# load_model = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size_encoder = 12800\n",
    "input_size_decoder = 15042\n",
    "output_size = 15042\n",
    "\n",
    "# input_size_encoder = len(src.vocab)\n",
    "# input_size_decoder = len(trg.vocab)\n",
    "# output_size = len(trg.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024  # Needs to be the same for both RNN's\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "# Tensorboard to get nice loss plot\n",
    "# writer = SummaryWriter(f\"runs/loss_plot\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'input_size_encoder is {input_size_encoder}')\n",
    "# print(f'input_size_decoder is {input_size_decoder}')\n",
    "# print(f'output_size is {output_size}')\n",
    "\n",
    "# sys.exit(0)\n",
    "\n",
    "encoder_net = Encoder(\n",
    "    input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
    ").to(device)\n",
    "\n",
    "decoder_net = Decoder(\n",
    "    input_size_decoder,\n",
    "    decoder_embedding_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    num_layers,\n",
    "    dec_dropout,\n",
    ").to(device)\n",
    "\n",
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# pad_idx = trg.vocab.stoi[\"<pad>\"]\n",
    "pad_idx = 1\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(12800, 300)\n",
       "    (rnn): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embedding): Embedding(15042, 300)\n",
       "    (rnn): LSTM(300, 1024, num_layers=2, dropout=0.5)\n",
       "    (fc): Linear(in_features=1024, out_features=15042, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model = True\n",
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_torch/pmindia.v1.hi-en.csv')\n",
    "str_punct = '''[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~।]'''\n",
    "\n",
    "df['eng_len'] = df['english_sentence'].str.count(' ')\n",
    "df['hi_len'] = df['hindi_sentence'].str.count(' ')\n",
    "# df.query('hi_len < 50 & eng_len < 50').shape\n",
    "\n",
    "df = df.query('hi_len < 50 & eng_len < 50')\n",
    "df = df.query('hi_len < eng_len * 1.5 & hi_len * 1.5 > eng_len')\n",
    "df['english_sentence'] = df['english_sentence'].apply(lambda x : re.sub(str_punct,'',x))\n",
    "df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x : re.sub(str_punct,'',x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'इस प्रकार एक स्वायशासी निकाय के रूप में जेएसके को बंद किया जा सकता है क्योंकि निधि के तौर पर उसका कामकाज विभाग द्वारा संभव है'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['hindi_sentence'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>eng_len</th>\n",
       "      <th>hi_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An advance is placed with the Medical Superint...</td>\n",
       "      <td>अग्रिम धन राशि इन अस्पतालों को चिकित्सा निरीक्...</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since the DoHFW provides funds to the hospital...</td>\n",
       "      <td>चूंकि स्वास्थ्य एवं परिवार कल्याण विभाग अस्पता...</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Managing Committee of RAN Society will meet to...</td>\n",
       "      <td>आरएएन सोसायटी की प्रबंध समिति सोसायटी पंजीकरण ...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In addition to this Health Minister’s Cancer P...</td>\n",
       "      <td>इसके अलावा स्वास्थ्य मंत्री के कैंसर रोगी निधि...</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The timeline required for this is one year</td>\n",
       "      <td>इसके लिए एक वर्ष का समय रखा गया है</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    english_sentence  \\\n",
       "0  An advance is placed with the Medical Superint...   \n",
       "1  Since the DoHFW provides funds to the hospital...   \n",
       "3  Managing Committee of RAN Society will meet to...   \n",
       "4  In addition to this Health Minister’s Cancer P...   \n",
       "5         The timeline required for this is one year   \n",
       "\n",
       "                                      hindi_sentence  eng_len  hi_len  \n",
       "0  अग्रिम धन राशि इन अस्पतालों को चिकित्सा निरीक्...       20      19  \n",
       "1  चूंकि स्वास्थ्य एवं परिवार कल्याण विभाग अस्पता...       19      22  \n",
       "3  आरएएन सोसायटी की प्रबंध समिति सोसायटी पंजीकरण ...       21      21  \n",
       "4  इसके अलावा स्वास्थ्य मंत्री के कैंसर रोगी निधि...       16      15  \n",
       "5                 इसके लिए एक वर्ष का समय रखा गया है        7       8  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence is \n",
      " \n",
      "इस प्रकार एक स्वायशासी निकाय के रूप में जेएसके को बंद किया जा सकता है क्योंकि निधि के तौर पर उसका कामकाज विभाग द्वारा संभव है\n",
      " \n",
      " translated sentence is \n",
      " \n",
      "an area of a Autonomous data can a can be administered as the of the the as a provisions of the the 6th Committee of the 6th of the Vice Committee profession for the profession of the <unk>\n"
     ]
    }
   ],
   "source": [
    "org_sentence = df['hindi_sentence'][10]\n",
    "tr_sentence = ' '.join(translate_sentence(model, org_sentence, src_loaded, trg_loaded, device))\n",
    "print(f'Original sentence is \\n \\n{org_sentence}\\n \\n translated sentence is \\n \\n{tr_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence is \n",
      " \n",
      "प्रधानमंत्री ने कहा कि भारत में केंद्र सरकार बुनियादी ढांचे पर ध्यान केंद्रित कर रही है\n",
      " \n",
      " translated sentence is \n",
      " \n",
      "conservation transparency in India Digital India initiative in the Prime Minister added India in India based India procurement infrastructure crop City based etc by the country infrastructure system etc in turn builds in a <unk>\n"
     ]
    }
   ],
   "source": [
    "org_sentence = df['hindi_sentence'][100]\n",
    "tr_sentence = ' '.join(translate_sentence(model, org_sentence, src_loaded, trg_loaded, device))\n",
    "print(f'Original sentence is \\n \\n{org_sentence}\\n \\n translated sentence is \\n \\n{tr_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence is \n",
      " \n",
      "आपका स्नेह मुझे कड़ी मेहनत करते रहने की प्रेरणा देता है”\n",
      " \n",
      " translated sentence is \n",
      " \n",
      "all have been given their words to their dear <unk>\n"
     ]
    }
   ],
   "source": [
    "org_sentence = df['hindi_sentence'][1000]\n",
    "tr_sentence = ' '.join(translate_sentence(model, org_sentence, src_loaded, trg_loaded, device))\n",
    "print(f'Original sentence is \\n \\n{org_sentence}\\n \\n translated sentence is \\n \\n{tr_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence is \n",
      " \n",
      "इसी कार्यक्रम में श्री नरेन्द्र मोदी मिर्जापुर मेडिकल कॉलेज की आधारशिला रखेंगे\n",
      " \n",
      " translated sentence is \n",
      " \n",
      "to <unk>\n"
     ]
    }
   ],
   "source": [
    "org_sentence = df['hindi_sentence'][4000]\n",
    "tr_sentence = ' '.join(translate_sentence(model, org_sentence, src_loaded, trg_loaded, device))\n",
    "print(f'Original sentence is \\n \\n{org_sentence}\\n \\n translated sentence is \\n \\n{tr_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence is \n",
      " \n",
      "6 ब्रिक्‍स शिखर सम्‍मेलन के पहले चक्र में सामूहिक रूप से हमारी अर्थव्‍यवस्‍थाओं ने आर्थिक संकट वैश्‍विक वित्‍तीय संकट से उबरने के इंजन के रूप में अपनी स्थिति मजबूत की है\n",
      " \n",
      " translated sentence is \n",
      " \n",
      "of the launch of BRICS <unk>\n"
     ]
    }
   ],
   "source": [
    "org_sentence = df['hindi_sentence'][6000]\n",
    "tr_sentence = ' '.join(translate_sentence(model, org_sentence, src_loaded, trg_loaded, device))\n",
    "print(f'Original sentence is \\n \\n{org_sentence}\\n \\n translated sentence is \\n \\n{tr_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence is \n",
      " \n",
      "हम सहकारी संघवाद की बात करते हैं लेकिन प्रतिस्‍पर्धी संघवाद की भी चर्चा करते हैं जहां राज्‍यों को निवेश और नौकरियों के लिए एकदूसरे से स्‍पर्धा करके श्रेष्‍ठता साबित करनी होगी\n",
      " \n",
      " translated sentence is \n",
      " \n",
      "we are also do do we continue but each other we so that we issues and investments continue to support with differences and support to cooperative and with the common common investments there heritage and other cooperative cooperative heritage there be shared their exports and achieving shared than the top\n"
     ]
    }
   ],
   "source": [
    "org_sentence = df['hindi_sentence'][7000]\n",
    "tr_sentence = ' '.join(translate_sentence(model, org_sentence, src_loaded, trg_loaded, device))\n",
    "print(f'Original sentence is \\n \\n{org_sentence}\\n \\n translated sentence is \\n \\n{tr_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49846, 4)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "      <th>eng_len</th>\n",
       "      <th>hi_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An advance is placed with the Medical Superint...</td>\n",
       "      <td>अग्रिम धन राशि इन अस्पतालों को चिकित्सा निरीक्...</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Since the DoHFW provides funds to the hospital...</td>\n",
       "      <td>चूंकि स्वास्थ्य एवं परिवार कल्याण विभाग अस्पता...</td>\n",
       "      <td>19</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Managing Committee of RAN Society will meet to...</td>\n",
       "      <td>आरएएन सोसायटी की प्रबंध समिति सोसायटी पंजीकरण ...</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In addition to this Health Minister’s Cancer P...</td>\n",
       "      <td>इसके अलावा स्वास्थ्य मंत्री के कैंसर रोगी निधि...</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The timeline required for this is one year</td>\n",
       "      <td>इसके लिए एक वर्ष का समय रखा गया है</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11258</th>\n",
       "      <td>They are helping me stay in close touch with p...</td>\n",
       "      <td>ये लोगों के करीब बने रहने में मेरी मदद कर रहे हैं</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11259</th>\n",
       "      <td>I learn a great deal from their suggestions an...</td>\n",
       "      <td>मैंने उनके सुझावों एवं शिकायतों से काफी कुछ सी...</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11260</th>\n",
       "      <td>We want to free our citizens from the burden o...</td>\n",
       "      <td>हम प्रत्‍येक कार्यालय में अपने नागरिकों को अत्...</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11262</th>\n",
       "      <td>We will set up a digital locker for every citi...</td>\n",
       "      <td>हम निजी दस्‍तावेजों को स्‍टोर करने के लिए प्रत...</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11263</th>\n",
       "      <td>We have set up Ebiz portal to make approvals f...</td>\n",
       "      <td>हमने कारोबारियों एवं नागरिकों के लिए अनुमोदनों...</td>\n",
       "      <td>28</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        english_sentence  \\\n",
       "0      An advance is placed with the Medical Superint...   \n",
       "1      Since the DoHFW provides funds to the hospital...   \n",
       "3      Managing Committee of RAN Society will meet to...   \n",
       "4      In addition to this Health Minister’s Cancer P...   \n",
       "5             The timeline required for this is one year   \n",
       "...                                                  ...   \n",
       "11258  They are helping me stay in close touch with p...   \n",
       "11259  I learn a great deal from their suggestions an...   \n",
       "11260  We want to free our citizens from the burden o...   \n",
       "11262  We will set up a digital locker for every citi...   \n",
       "11263  We have set up Ebiz portal to make approvals f...   \n",
       "\n",
       "                                          hindi_sentence  eng_len  hi_len  \n",
       "0      अग्रिम धन राशि इन अस्पतालों को चिकित्सा निरीक्...       20      19  \n",
       "1      चूंकि स्वास्थ्य एवं परिवार कल्याण विभाग अस्पता...       19      22  \n",
       "3      आरएएन सोसायटी की प्रबंध समिति सोसायटी पंजीकरण ...       21      21  \n",
       "4      इसके अलावा स्वास्थ्य मंत्री के कैंसर रोगी निधि...       16      15  \n",
       "5                     इसके लिए एक वर्ष का समय रखा गया है        7       8  \n",
       "...                                                  ...      ...     ...  \n",
       "11258  ये लोगों के करीब बने रहने में मेरी मदद कर रहे हैं        9      11  \n",
       "11259  मैंने उनके सुझावों एवं शिकायतों से काफी कुछ सी...        9       9  \n",
       "11260  हम प्रत्‍येक कार्यालय में अपने नागरिकों को अत्...       15      16  \n",
       "11262  हम निजी दस्‍तावेजों को स्‍टोर करने के लिए प्रत...       19      25  \n",
       "11263  हमने कारोबारियों एवं नागरिकों के लिए अनुमोदनों...       28      32  \n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence is \n",
      " \n",
      "ये लोगों के करीब बने रहने में मेरी मदद कर रहे हैं\n",
      " \n",
      " translated sentence is \n",
      " \n",
      "it of social security in this field of adversity in the society also also be an live in the also in their life as festival as to the festival of life child as child it as child and savings to be it as to the people savings to the women\n"
     ]
    }
   ],
   "source": [
    "org_sentence = df['hindi_sentence'][11258]\n",
    "tr_sentence = ' '.join(translate_sentence(model, org_sentence, src_loaded, trg_loaded, device))\n",
    "print(f'Original sentence is \\n \\n{org_sentence}\\n \\n translated sentence is \\n \\n{tr_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('src.pickle', 'wb') as handle:\n",
    "#     pickle.dump(src, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('trg.pickle', 'wb') as handle:\n",
    "#     pickle.dump(trg, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "with open('src.pickle', 'rb') as handle:\n",
    "    src_loaded = pickle.load(handle)\n",
    "\n",
    "with open('trg.pickle', 'rb') as handle:\n",
    "    trg_loaded = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size_encoder is 12800\n",
      "input_size_decoder is 15042\n",
      "output_size is 15042\n",
      "pad_idx is 1\n",
      "Total time taken for data loading was >> 0.0006196498870849609\n"
     ]
    }
   ],
   "source": [
    "# sentence = 'अग्रिम धन राशि इन अस्पतालों को चिकित्सा निरीक्षकों को दी जाएगी जो हर मामले को देखते हुए सहायता प्रदान करेंगे'\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "print(f'input_size_encoder is {input_size_encoder}')\n",
    "print(f'input_size_decoder is {input_size_decoder}')\n",
    "print(f'output_size is {output_size}')\n",
    "print(f'pad_idx is {pad_idx}')\n",
    "\n",
    "print(f'Total time taken for data loading was >> {time.time() -st}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_test = datasets.TranslationDataset(\n",
    "     path='./data_torch/data_sm', exts=('.hi', '.en'),\n",
    "     fields=(src, trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.metrics import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bleu(data, model, german, english, device):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "    for example in data:\n",
    "        try:\n",
    "            src = vars(example)[\"src\"]\n",
    "            trg = vars(example)[\"trg\"]\n",
    "\n",
    "            prediction = translate_sentence(model, src, german, english, device)\n",
    "            prediction = prediction[:-1] # remove <eos> token\n",
    "\n",
    "            targets.append([trg])\n",
    "            outputs.append(prediction)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e, example)\n",
    "    scores = 0\n",
    "    try:\n",
    "        scores = bleu_score(outputs, targets)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'got the exception from bleu score , >>{e}')\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "score_test = bleu(mt_test[1:400], model, src_loaded, trg_loaded, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got the exception from bleu score , >>index 4 is out of bounds for dimension 0 with size 4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-7858e1a34b76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mscore_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmt_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_loaded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_loaded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Bleu score for test data is {score_test*100:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Total time taken for score_test was >> {time.time() -st}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "print(f\"Bleu score for test data is {score_test*100:.2f}\")\n",
    "print(f'Total time taken for score_test was >> {time.time() -st}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for dimension 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-8b95dcc84854>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscore_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmt_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Bleu score for train data is {score_train*100:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Total time taken for score_train was >> {time.time() -st}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/disk_ext/nlp/seq2seq_vanilla/utils.py\u001b[0m in \u001b[0;36mbleu\u001b[0;34m(data, model, german, english, device)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbleu_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/inltk/lib/python3.7/site-packages/torchtext/data/metrics.py\u001b[0m in \u001b[0;36mbleu_score\u001b[0;34m(candidate_corpus, references_corpus, max_n, weights)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidate_counter\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TODO: no need to loop through the whole counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mtotal_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcandidate_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_counts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for dimension 0 with size 4"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "score_train = bleu(mt_train[1:100], model, src, trg, device)\n",
    "print(f\"Bleu score for train data is {score_train*100:.2f}\")\n",
    "print(f'Total time taken for score_train was >> {time.time() -st}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint\n",
    "from inltk.inltk import tokenize\n",
    "# from inltk.inltk import setup\n",
    "# setup('hi')\n",
    "from torchtext import datasets\n",
    "from torchtext import data\n",
    "\n",
    "\n",
    "spacy_ger = spacy.load(\"de_core_news_sm\")\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_hi(text):\n",
    "    return tokenize(text, \"hi\")\n",
    "\n",
    "def tokenize_eng(text):\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "\n",
    "src = data.Field(tokenize=tokenize_hi)\n",
    "trg = data.Field(tokenize=tokenize_eng)\n",
    "mt_train = datasets.TranslationDataset(\n",
    "     path='./data_torch/data_sm', exts=('.hi', '.en'),\n",
    "     fields=(src, trg))\n",
    "\n",
    "src.build_vocab(mt_train, max_size=20000, min_freq=2)\n",
    "trg.build_vocab(mt_train, max_size=20000, min_freq=2)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N) where N is batch size\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape: (seq_length, N, hidden_size)\n",
    "\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # outputs shape: (1, N, hidden_size)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(trg.vocab)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # Grab the first input to the Decoder which will be <SOS> token\n",
    "        x = target[0]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Use previous hidden, cell as context from encoder at start\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "\n",
    "            outputs[t] = output\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs\n",
    "\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "# Model hyperparameters\n",
    "load_model = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size_encoder = len(src.vocab)\n",
    "input_size_decoder = len(trg.vocab)\n",
    "output_size = len(trg.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024  # Needs to be the same for both RNN's\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "# Tensorboard to get nice loss plot\n",
    "writer = SummaryWriter(f\"runs/loss_plot\")\n",
    "step = 0\n",
    "\n",
    "train_iter = data.BucketIterator(\n",
    "     dataset=mt_train, batch_size=32,\n",
    "     sort_key=lambda x: data.interleave_keys(len(x.src), len(x.trg)))\n",
    "\n",
    "encoder_net = Encoder(\n",
    "    input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
    ").to(device)\n",
    "\n",
    "decoder_net = Decoder(\n",
    "    input_size_decoder,\n",
    "    decoder_embedding_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    num_layers,\n",
    "    dec_dropout,\n",
    ").to(device)\n",
    "\n",
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "pad_idx = trg.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "load_model = False\n",
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "\n",
    "sentence = 'अग्रिम धन राशि इन अस्पतालों को चिकित्सा निरीक्षकों को दी जाएगी जो हर मामले को देखते हुए सहायता प्रदान करेंगे'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch is 0/1000]\n",
      "=> Saving checkpoint\n",
      "Translated sentence is: \n",
      " <unk>\n",
      "[Epoch is 1/1000]\n",
      "=> Saving checkpoint\n",
      "Translated sentence is: \n",
      " <unk>\n",
      "[Epoch is 2/1000]\n",
      "=> Saving checkpoint\n",
      "Translated sentence is: \n",
      " <unk>\n",
      "[Epoch is 3/1000]\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8e4c346ae7b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'[Epoch is {epoch}/{num_epochs}]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/disk_ext/nlp/seq2seq_torch/utils.py\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(state, filename)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"my_checkpoint.pth.tar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=> Saving checkpoint\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/inltk/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/inltk/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/inltk/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f'[Epoch is {epoch}/{num_epochs}]')\n",
    "    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint)\n",
    "    \n",
    "    model.eval()\n",
    "    translated_sentence = translate_sentence(\n",
    "        model=model, sentence= sentence, german=src, english=trg, device=device, max_length=50\n",
    "    )\n",
    "    translated_sentence =  ' '.join(translated_sentence)\n",
    "    print(f'Translated sentence is: \\n {translated_sentence}')\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_iter):\n",
    "        inp_data = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "        \n",
    "        output = model(inp_data, target)\n",
    "        \n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        writer.add_scalar(\"training_loss\", loss, global_step=step)\n",
    "        step+=1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '''An advance is placed with  [ the Medical Superintendents of such hospitals who then provide assistance on a case to case basis.'''\n",
    "\n",
    "\n",
    "re.sub('''[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~।]''','',txt)\n",
    "\n",
    "df.english_sentence[0]\n",
    "\n",
    "\n",
    "df.head(1)['hindi_sentence'][0]\n",
    "\n",
    "# df = df.sample(frac=0.01)\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# # create train and validation set \n",
    "# train, val = train_test_split(df[['english_sentence', 'hindi_sentence']], test_size=0.2)\n",
    "\n",
    "# train.to_csv(\"./data_torch/train.csv\", index=False)\n",
    "# val.to_csv(\"./data_torch/val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "from utils import translate_sentence, bleu, save_checkpoint, load_checkpoint\n",
    "from inltk.inltk import tokenize\n",
    "\n",
    "# from inltk.inltk import setup\n",
    "# setup('hi')\n",
    "\n",
    "from torchtext import datasets\n",
    "\n",
    "from torchtext import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_ger = spacy.load(\"de_core_news_sm\")\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_hi(text):\n",
    "    return tokenize(text, \"hi\")\n",
    "\n",
    "def tokenize_eng(text):\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = data.Field(tokenize=tokenize_hi)\n",
    "trg = data.Field(tokenize=tokenize_eng)\n",
    "mt_train = datasets.TranslationDataset(\n",
    "     path='./data_torch/data', exts=('.hi', '.en'),\n",
    "     fields=(src, trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "src.build_vocab(mt_train, max_size=10000, min_freq=2)\n",
    "trg.build_vocab(mt_train, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(mt_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N) where N is batch size\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape: (seq_length, N, hidden_size)\n",
    "\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n",
    "        # is 1 here because we are sending in a single word and not a sentence\n",
    "        x = x.unsqueeze(0)\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # outputs shape: (1, N, hidden_size)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "\n",
    "        # predictions shape: (1, N, length_target_vocabulary) to send it to\n",
    "        # loss function we want it to be (N, length_target_vocabulary) so we're\n",
    "        # just gonna remove the first dim\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(trg.vocab)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # Grab the first input to the Decoder which will be <SOS> token\n",
    "        x = target[0]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # Use previous hidden, cell as context from encoder at start\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "\n",
    "            # Store next output prediction\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # With probability of teacher_force_ratio we take the actual next word\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            # Teacher Forcing is used so that the model gets used to seeing\n",
    "            # similar inputs at training and testing time, if teacher forcing is 1\n",
    "            # then inputs at test time might be completely different than what the\n",
    "            # network is used to. This was a long comment.\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "# Model hyperparameters\n",
    "load_model = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size_encoder = len(src.vocab)\n",
    "input_size_decoder = len(trg.vocab)\n",
    "output_size = len(trg.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024  # Needs to be the same for both RNN's\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "# Tensorboard to get nice loss plot\n",
    "writer = SummaryWriter(f\"runs/loss_plot\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data.BucketIterator(\n",
    "     dataset=mt_train, batch_size=32,\n",
    "     sort_key=lambda x: data.interleave_keys(len(x.src), len(x.trg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_net = Encoder(\n",
    "    input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_net = Decoder(\n",
    "    input_size_decoder,\n",
    "    decoder_embedding_size,\n",
    "    hidden_size,\n",
    "    output_size,\n",
    "    num_layers,\n",
    "    dec_dropout,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = trg.vocab.stoi[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 32]\n",
       "\t[.src]:[torch.LongTensor of size 66x32]\n",
       "\t[.trg]:[torch.LongTensor of size 48x32]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# usage\n",
    "next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "\n",
    "sentence = 'अग्रिम धन राशि इन अस्पतालों को चिकित्सा निरीक्षकों को दी जाएगी जो हर मामले को देखते हुए सहायता प्रदान करेंगे'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch is 0/1]\n",
      "=> Saving checkpoint\n",
      "Translated sentence is: \n",
      " ['exchanging', 'Munde', 'Design', 'transferee', 'Said', 'Said', 'Said', 'Said', 'Said', 'before', 'ultimate', 'customer', 'stepping', 'stepping', 'Partnership', 'allows', 'allows', 'repository', 'repository', 'BARC', 'discretion', 'Sinchai', 'food', 'Sinchai', 'amounts', 'Italian', 'benami', 'Colleagues', 'Cyclone', 'Braille', 'Said', 'Said', 'Said', 'Said', 'Said', 'before', 'ultimate', 'customer', 'stepping', 'stepping', 'Partnership', 'allows', 'allows', 'repository', 'repository', 'BARC', 'discretion', 'Sinchai', 'food', 'Sinchai']\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f'[Epoch is {epoch}/{num_epochs}]')\n",
    "    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint)\n",
    "    \n",
    "    model.eval()\n",
    "    translated_sentence = translate_sentence(\n",
    "        model=model, sentence= sentence, german=src, english=trg, device=device, max_length=50\n",
    "    )\n",
    "    \n",
    "    print(f'Translated sentence is: \\n {translated_sentence}')\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_iter):\n",
    "        inp_data = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "        \n",
    "        output = model(inp_data, target)\n",
    "        \n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        writer.add_scalar(\"training_loss\", loss, global_step=step)\n",
    "        step+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent = 'अग्रिम धन राशि इन अस्पतालों को चिकित्सा निरीक्षकों को दी जाएगी जो हर मामले को देखते हुए सहायता प्रदान करेंगे'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "translate_sentence(model, test_sent, src, trg, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'अग्रिम धन राशि इन अस्पतालों को चिकित्सा निरीक्षक ों को दी जाएगी , जो हर मामले को देखते हुए सहायता प्रदान करेंगे ।'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([i.lstrip('▁') for i in list(mt_train[0].__dict__.values())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''.lstrip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_test = data.Field(tokenize=tokenize_hi)\n",
    "trg_test = data.Field(tokenize=tokenize_eng)\n",
    "mt_test = datasets.TranslationDataset(\n",
    "     path='./data_torch/data_val', exts=('.hi', '.en'),\n",
    "     fields=(src, trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(mt_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# raw_data = './data_torch/pmindia.v1.hi-en.csv'\n",
    "\n",
    "# df = pd.read_csv(raw_data)\n",
    "# # remove very long sentences and sentences where translations are \n",
    "# # not of roughly equal length\n",
    "# df['eng_len'] = df['english_sentence'].str.count(' ')\n",
    "# df['hi_len'] = df['hindi_sentence'].str.count(' ')\n",
    "\n",
    "# df = df.query('hi_len < 50 & eng_len < 50')\n",
    "# df = df.query('hi_len < eng_len * 1.5 & hi_len * 1.5 > eng_len')\n",
    "# import numpy as np\n",
    "\n",
    "# np.savetxt(r'./data_torch/data_val.en', df.english_sentence.tail(100), fmt='%s')\n",
    "# np.savetxt(r'./data_torch/data_val.hi', df.hindi_sentence.tail(100), fmt='%s')\n",
    "# # df.sample(frac=0.1).shape\n",
    "\n",
    "# # df = df.sample(frac=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([['▁31.', '▁दोनों', '▁नेताओं', '▁ने', '▁डब्ल्यू', 'टी', 'ओ', '▁संबंधी', '▁मुद्दों', '▁पर', '▁एक', '▁द्विपक्ष', 'ीय', '▁सलाहकार', '▁तंत्र', '▁शुरू', '▁करने', '▁के', '▁फैसले', '▁का', '▁स्वागत', '▁करते', '▁हुए', '▁इसे', '▁वैश्विक', '▁व्यापार', '▁बातचीत', '▁के', '▁संदर्भ', '▁में', '▁सहयोग', '▁बढ़ाने', '▁के', '▁लिए', '▁एक', '▁सकारात्मक', '▁कदम', '▁बताया', '।'], ['31', '.', 'The', 'leaders', 'welcomed', 'the', 'decision', 'to', 'launch', 'a', 'bilateral', 'consultative', 'mechanism', 'on', 'WTO', '-', 'related', 'issues', 'as', 'a', 'positive', 'step', 'for', 'enhancing', 'coordination', 'in', 'the', 'context', 'of', 'global', 'trade', 'talks', '.']])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt_test[0].__dict__.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu score 0.00\n"
     ]
    }
   ],
   "source": [
    "score = bleu(mt_test[1:100], model, src, trg, device)\n",
    "print(f\"Bleu score {score*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
    "\n",
    "    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    translated_sentence = translate_sentence(\n",
    "        model, sentence, german, english, device, max_length=50\n",
    "    )\n",
    "\n",
    "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        # Get input and targets and get to cuda\n",
    "        inp_data = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "\n",
    "        # Forward prop\n",
    "        output = model(inp_data, target)\n",
    "\n",
    "        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n",
    "        # doesn't take input in that form. For example if we have MNIST we want to have\n",
    "        # output to be: (N, 10) and targets just (N). Here we can view it in a similar\n",
    "        # way that we have output_words * batch_size that we want to send in into\n",
    "        # our cost function, so we need to do some reshapin. While we're at it\n",
    "        # Let's also remove the start token while we're at it\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Back prop\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip to avoid exploding gradient issues, makes sure grads are\n",
    "        # within a healthy range\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "        # Gradient descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Plot to tensorboard\n",
    "        writer.add_scalar(\"Training loss\", loss, global_step=step)\n",
    "        step += 1\n",
    "\n",
    "\n",
    "score = bleu(test_data[1:100], model, german, english, device)\n",
    "print(f\"Bleu score {score*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
